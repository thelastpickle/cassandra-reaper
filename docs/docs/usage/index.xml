<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Using Reaper on Reaper: Easy Repair Management for Apache Cassandra</title>
    <link>http://cassandra-reaper.io/docs/usage/</link>
    <description>Recent content in Using Reaper on Reaper: Easy Repair Management for Apache Cassandra</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="http://cassandra-reaper.io/docs/usage/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Adding a Cluster</title>
      <link>http://cassandra-reaper.io/docs/usage/add_cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://cassandra-reaper.io/docs/usage/add_cluster/</guid>
      <description>Enter an address of one of the nodes in the cluster, then click Add Cluster Reaper will contact that node and find the rest of the nodes in the cluster automatically.
Once successfully completed, the Cluster&amp;rsquo;s health will be displayed.
If JMX authentication is required and all clusters share the same credentials, they have to be filled in the Reaper YAML file, under jmxAuth (see the configuration reference).
Specific JMX credentials per cluster Since 1.</description>
    </item>
    
    <item>
      <title>Checking a Cluster&#39;s Health</title>
      <link>http://cassandra-reaper.io/docs/usage/health/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://cassandra-reaper.io/docs/usage/health/</guid>
      <description>Dashboard When a cluster has been added to Reaper it will be displayed in the dashboard.
Node View Clicking on one of the nodes will open a dialog box containing details of the node&amp;rsquo;s state.</description>
    </item>
    
    <item>
      <title>Running a Cluster Repair</title>
      <link>http://cassandra-reaper.io/docs/usage/single/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://cassandra-reaper.io/docs/usage/single/</guid>
      <description>Reaper has the ability to launch a once-off repair on a cluster. This can be done in the following way.
Start a New Repair Click the repair menu item on the left side to navigate to the Repair page. Click Start a new repair to open the repair details form.
Fill in the Details Enter values for the keyspace, tables, owner and other fields and click the Repair button. See the table below for further information on the details for each field.</description>
    </item>
    
    <item>
      <title>Scheduling a Cluster Repair</title>
      <link>http://cassandra-reaper.io/docs/usage/schedule/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://cassandra-reaper.io/docs/usage/schedule/</guid>
      <description>Reaper has the ability to create and manage repair schedules for a cluster. This can be done in the following way.
Setup a Repair Schedule Click the schedule menu item on the left side to navigate to the Schedules page. Click Add schedule to open the schedule details form.
Fill in the Details Enter values for the keyspace, tables, owner and other fields and click Add Schedule button. The details for adding a schedule are similar to the details for Repair form except the &amp;ldquo;Cause&amp;rdquo; field is replaced with three fields; &amp;ldquo;Start time&amp;rdquo;, &amp;ldquo;Interval in days&amp;rdquo; and &amp;ldquo;Percent unrepaired threshold&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Multi DCs with One Reaper</title>
      <link>http://cassandra-reaper.io/docs/usage/multi_dc_non-distributed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://cassandra-reaper.io/docs/usage/multi_dc_non-distributed/</guid>
      <description>Reaper can operate clusters which has a multi datacenter deployment. The datacenterAvailability setting in the Reaper YAML file indicates to Reaper its deployment in relation to cluster data center network locality.
Single Reaper instance with JMX accessible for all DCs In the case where the JMX port is accessible (with or without authentication) from the running Reaper instance for all nodes in all DCs, it is possible to have a single instance of Reaper handle one or multiple clusters by using the following setting in the configuration yaml file :</description>
    </item>
    
    <item>
      <title>Multi DCs with Multi Reapers</title>
      <link>http://cassandra-reaper.io/docs/usage/multi_dc_distributed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://cassandra-reaper.io/docs/usage/multi_dc_distributed/</guid>
      <description>Multiple Reaper instances can operate clusters which have multi datacenter deployment. Multiple Reaper instances, also known as Distributed mode, can only be used when using the Apache Cassandra backend. Using multiple Reaper instances allows improved availability and fault tolerance. It is more likely that a Reaper UI is available via one of the Reaper instances, and that scheduled repairs are executed by one of the running Reaper instances.
The datacenterAvailability setting in the Reaper YAML file indicates to Reaper its deployment in relation to cluster data center network locality.</description>
    </item>
    
    <item>
      <title>Sidecar Mode</title>
      <link>http://cassandra-reaper.io/docs/usage/sidecar_mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://cassandra-reaper.io/docs/usage/sidecar_mode/</guid>
      <description>Sidecar Mode is a way of deploying Reaper for Apache Cassandra with one reaper instance for each node in the cluster. The name &amp;ldquo;Sidecar&amp;rdquo; comes from the Sidecar Pattern which describes a mechanism for co-locating an auxiliary service with its supported application. See also Design Patterns for Container-based Distributed Systems. It is a pattern that is often used in Kubernetes, where the main application and the sidecar application are deployed as separate containers in the same pod.</description>
    </item>
    
  </channel>
</rss>
