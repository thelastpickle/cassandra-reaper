[
{
	"uri": "http://cassandra-reaper.io/docs/usage/add_cluster/",
	"title": "Adding a Cluster",
	"tags": [],
	"description": "",
	"content": "Enter an address of one of the nodes in the cluster, then click Add Cluster Reaper will contact that node and find the rest of the nodes in the cluster automatically.\nOnce successfully completed, the Cluster\u0026rsquo;s health will be displayed.\nIf JMX authentication is required and all clusters share the same credentials, they have to be filled in the Reaper YAML file, under jmxAuth (see the configuration reference).\nSpecific JMX credentials per cluster Since 1.1.0\nIf the clusters require authentication for JMX access, credentials will need to be filled in the reaper yaml configuration file. See the jmxCredentials setting in the configuration reference for detailed informations.\nWhen using jmxCredentials (if each cluster has specific credentials), the name of the cluster will need to be indicated in the seed node address.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/metrics/dashboards/",
	"title": "Dashboards",
	"tags": [],
	"description": "",
	"content": "Dashboards There are many ways of visualising the metrics Reaper exposes. Here is just a few examples to get you started.\nWe are going to use Grafana to visualize metrics fetched from a Prometheus backed. We will also assume the metrics were relabeled.\nRepair progress One of the simplest things to visualize is repair progress. We can do that using a simple Gauge, where we use use the following query:\nio_cassandrareaper_service_RepairRunner_repairProgress To make the panel more pretty, we can enter the following in the Legend field:\n{{cluster}} - {{keyspace}} With two keyspaces doing repairs, we should see two dials, one for each keyspace:\nWe can now switch the Gauge visualisation for a Stat one, which will give us the repair progress, but also a small graph showing the progress over time:\nTo monitor the repair progress in terms of the number of repaired segments, we can use the following query:\nio_cassandrareaper_service_RepairRunner_segmentsDone Again, we put this query into the Stat panel, but we also select Last for the Calc field in the Display part of the Visualization tab. We should get a chart that looks like this:\nSegment duration When monitoring repairs, we would like to know how much time it takes to repair a segment. As a rule of thumb, we would like each segment to take 10 to 15 minutes. This indicates the segments are big enough to not waste the overhead of a repair session and minimise over-streaming, while not being too large to risk streaming timeouts.\nTo check segment durations, we will use the SegmentRunner_runRepair metric. This metric is a timer covering the duration from the moment a SegmentRunner wakes up to repair a segment until the segment finishes. In this case, we will use a Graph and feed it the following querry:\nio_cassandrareaper_service_SegmentRunner_repairing{quantile=\u0026#34;0.5\u0026#34;} Unlike previous examples, here we add a filter to explicitly pick the 0.5th percentile because we are interested in the majority of the segments, not just the longest ones. Reaper already gives us this duration in seconds, so we pick Seconds (s) as a Unit for the Left Y axis in the Axes section of the Visualization tab. We should end up with a graph that looks like this:\nWe see our segments are taking ~30 seconds, which is way below the desired ~10 minutes. We should tweak our repairs to use less segments.\nSegments per hour For the last two graphs, we\u0026rsquo;ll try something harder. We\u0026rsquo;ll try to plot the number of segments repaired per hour.\nWe create another Graph panel and feed it a query like this:\n(increase(io_cassandrareaper_service_RepairRunner_segmentsDone[1h])) The above simply means we ask Prometheus to give us the increase of the given metric in a 1 hour window.\nNext, we enter 1h as a Min step in the query tab (it\u0026rsquo;s next to the Legend field), and we set 24h as a Relative time. In the Visualization tab, we select to draw Bars and turn on stacking. We should end up with a graph like this:\nNote that the panel now says it tracks the last 24 hours in the top right corner - this is because of the relative time we selected. Also, even though it\u0026rsquo;s not obvious from the graph, Grafana will draw a bar only after that hour passes. In other words, you might need to wait a bit before you start to see your bars.\nSegments in the last hour Finally, we can also plot the number of segments repaired in the last hour only. We use the same query, min step and Relative time as before, but this time we put it into a Bar Gauge with a vertical orientation. We should see this:\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/",
	"title": "Documentation",
	"tags": [],
	"description": "",
	"content": "Welcome to the Reaper for Apache Cassandra documentation! Overview Reaper is an open source tool that aims to schedule and orchestrate repairs of Apache Cassandra clusters.\nIt improves the existing nodetool repair process by:\nSplitting repair jobs into smaller tunable segments. Handling back-pressure through monitoring running repairs and pending compactions. Adding ability to pause or cancel repairs and track progress precisely. Reaper ships with a REST API, a command line tool and a web UI.\nThis documentation includes instructions on how build, install, and configure Reaper correctly.\nCompatibility Reaper supports all versions of Apache Cassandra ranging from 1.2 to the latest release. Incremental repair is supported for versions from 2.1 and above.\nA single instance of Reaper can handle repairs for clusters running different Apache Cassandra versions.\nDownload and Installation Head over to the Downloads section to download the version of Reaper that is compatible with your system.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/metrics/graphite/",
	"title": "Graphite Reporter",
	"tags": [],
	"description": "",
	"content": "Graphite Reporter Reaper can be configured to periodically report metrics to a Graphite host. This can be done using the following properties in the Reaper configuration YAML file.\nMetrics: frequency: 1 minute reporters: - type: graphite host: \u0026lt;host_address\u0026gt; port: \u0026lt;port_number\u0026gt; prefix: \u0026lt;prefix\u0026gt; Where:\nhost_address is hostname of the Graphite server to report to. port_number is port of the Graphite server to report to. prefix is prefix for Metric key names that are reported to the Graphite server. Typically this will be the hostname sending the metrics to the Graphite server. "
},
{
	"uri": "http://cassandra-reaper.io/docs/download/install/",
	"title": "Install and Run",
	"tags": [],
	"description": "",
	"content": "Requirements Since Reaper v4, Java 11 is required to compile and run it. More recent versions of the JDK should also be able to run the compiled version of Reaper.\nRunning Reaper using the jar After modifying the resource/cassandra-reaper.yaml config file, Reaper can be started using the following command line :\njava -jar target/cassandra-reaper-X.X.X.jar server resource/cassandra-reaper.yaml Once started, the UI can be accessed through : http://127.0.0.1:8080/webui/\nReaper can also be accessed using the REST API exposed on port 8080, or using the command line tool bin/spreaper\nInstalling and Running as a Service We provide prebuilt packages for reaper on Cloudsmith.\nRPM Install (CentOS, Fedora, RHEK) Grab the RPM from GitHub and install using the rpm command:\nsudo rpm -ivh reaper-*.*.*.x86_64.rpm Using yum (stable releases) 1/ Run the following to install the repo:\ncurl -1sLf \\ \u0026#39;https://dl.cloudsmith.io/public/thelastpickle/reaper/setup.rpm.sh\u0026#39; \\ | sudo -E bash 2/ Install reaper :\nsudo yum install reaper In case of problem, check the alternate procedure on cloudsmith.io.\nUsing yum (development builds) 1/ Run the following to install the repo:\ncurl -1sLf \\ \u0026#39;https://dl.cloudsmith.io/public/thelastpickle/reaper-beta/setup.rpm.sh\u0026#39; \\ | sudo -E bash 2/ Install reaper :\nsudo yum install reaper In case of problem, check the alternate procedure on cloudsmith.io.\nDEB (Debian based distros like Ubuntu) After downloading the DEB package, install using the dpkg command:\nsudo dpkg -i reaper_*.*.*_amd64.deb Using apt-get (stable releases) 1/ Using the command line, run the following:\ncurl -1sLf \\ \u0026#39;https://dl.cloudsmith.io/public/thelastpickle/reaper/setup.deb.sh\u0026#39; \\ | sudo -E bash 2/ Install reaper :\nsudo apt-get update sudo apt-get install reaper In case of problem, check the alternate procedure on cloudsmith.io.\nUsing apt-get (development builds) 1/ Using the command line, run the following command:\ncurl -1sLf \\ \u0026#39;https://dl.cloudsmith.io/public/thelastpickle/reaper-beta/setup.deb.sh\u0026#39; \\ | sudo -E bash 2/ Install reaper :\nsudo apt-get update sudo apt-get install reaper In case of problem, check the alternate procedure on cloudsmith.io.\nService Configuration The yaml file used by the service is located at /etc/cassandra-reaper/cassandra-reaper.yaml and alternate config templates can be found under /etc/cassandra-reaper/configs. It is recommended to create a new file with your specific configuration and symlink it as /etc/cassandra-reaper/cassandra-reaper.yaml to avoid your configuration from being overwritten during upgrades.\nAdapt the config file to suit your setup and then run sudo service cassandra-reaper start.\nLog files can be found at /var/log/cassandra-reaper.log and /var/log/cassandra-reaper.err.\nStop the service by running : sudo service cassandra-reaper stop\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/backends/memory/",
	"title": "Local Backend",
	"tags": [],
	"description": "",
	"content": "To use in memory storage as the storage type for Reaper, the storageType setting must be set to memory in the Reaper configuration YAML file. Note that the in memory storage is enabled by default. An example of how to configure Reaper with In-Menory storage can be found in the cassandra-reaper-memory.yaml.\nstorageType: memory persistenceStoragePath: /var/lib/cassandra-reaper/storage In-memory storage is volatile and as such all registered cluster, column families and repair information will be lost upon service restart. This storage setting is intended for testing purposes only.\nStarting from 3.6.0, persistenceStoragePath is required for memory storage type. This enable lightweight deployments of Reaper, without requiring the use of a Cassandra database. It will store the data locally and reload them consistently upon startup.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/metrics/prometheus/",
	"title": "Prometheus",
	"tags": [],
	"description": "",
	"content": "Prometheus Reaper exposes all its metrics in a Prometheus-ready format under the /prometheusMetrics endpoint on the admin port.\nIt\u0026rsquo;s fairly straightforward to configure Prometheus to grab them. The config can look something like:\nscrape_configs: - job_name: \u0026#39;reaper\u0026#39; metrics_path: \u0026#39;/prometheusMetrics\u0026#39; scrape_interval: 5s static_configs: - targets: [\u0026#39;host.docker.internal:8081\u0026#39;] The host.docker.internal tells a Prometheus instance running inside a docker container to connect to the host\u0026rsquo;s 8081 port where Raper runs from a JAR. "
},
{
	"uri": "http://cassandra-reaper.io/docs/download/building/",
	"title": "Building from Source",
	"tags": [],
	"description": "",
	"content": "Building Install Packages Debian packages and RPMs can be built from this project using Make, for example:\nmake deb make rpm Building JARs from source Since Reaper v4, Java 11 is required to compile and run it. More recent versions of the JDK should also be able to run the compiled version of Reaper. Node JS v10 is still required to build the UI as part of the maven build, with npm v9.\nTo build use the following command:\nmvn clean package Building Docker Image from source See the Docker section for more details.\nBuilding Using Docker To simplify the build toolchain it\u0026rsquo;s possible to build everything using Docker itself. This is the process used to build the release binary artifacts from jar files to debian packages.\nBuilding Reaper packages requires quite a few dependencies, especially when making changes to the web interface code. In an effort to simplify the build process, Dockerfiles have been created that implement the build actions required to package Reaper.\nTo build the JAR and other packages which are then placed in the src/packages directory run the following commands:\ncd src/packaging/docker-build docker-compose build docker-compose run build "
},
{
	"uri": "http://cassandra-reaper.io/docs/backends/cassandra/",
	"title": "Cassandra Backend",
	"tags": [],
	"description": "",
	"content": "To use Apache Cassandra as the persistent storage for Reaper, the storageType setting must be set to cassandra in the Reaper configuration YAML file. In addition, the connection details for the Apache Cassandra cluster being used to store Reaper data must be specified in the configuration YAML file. An example of how to configure Cassandra as persistent storage for Reaper can be found in the cassandra-reaper-cassandra.yaml.\nstorageType: cassandra cassandra: clusterName: \u0026#34;test\u0026#34; contactPoints: [\u0026#34;127.0.0.1\u0026#34;] keyspace: reaper_db If you\u0026rsquo;re using authentication or SSL:\nstorageType: cassandra cassandra: clusterName: \u0026#34;test\u0026#34; contactPoints: [\u0026#34;127.0.0.1\u0026#34;] keyspace: reaper_db authProvider: type: plain-text username: cassandra password: cassandra ssl: type: jdk The Apache Cassandra backend is the only deployment that allows multiple Reaper instances to operate concurrently. This provides high availability and allows to repair multi DC clusters.\nTo run Reaper using the Cassandra backend, create a reaper_db keyspace with an appropriate placement strategy. This is installation specific, and names of the data centers in the cluster that will host the Reaper data must be specified. For example:\nCREATE KEYSPACE reaper_db WITH replication = {\u0026#39;class\u0026#39;: \u0026#39;NetworkTopologyStrategy\u0026#39;, \u0026#39;\u0026lt;data_center\u0026gt;\u0026#39;: 3}; Where:\n\u0026lt;data_center\u0026gt; is the name of the Cassandra data center that will contain the keyspace replicas. When operating Reaper in a production environment, it is recommended that:\nAn RF (Replication Factor) of 3 be used in each data center for the reaper_db keyspace. This is to ensure that all Reaper state data is still available should a node in the cluster be unavailable. The NetworkTopologyStrategy should be used for the replication strategy of the keyspace. This is because LOCAL_* requests will fail if the SimpleNetworkingStrategy is used in an environment where there is more than one data center defined. Schema initialization and migration will be done automatically upon startup.\nSometimes it\u0026rsquo;s not possible for Cassandra nodes to broadcast addresses that will work for each and every client; for instance, they might broadcast private IPs because most clients are in the same network, but a particular client could be on another network and go through a router. For such cases, you can configure a custom address translator that will perform additional address translation based on configured mapping.\nstorageType: cassandra cassandra: clusterName: \u0026#34;test\u0026#34; contactPoints: [\u0026#34;node1.cassandra.reaper.io\u0026#34;, \u0026#34;node2.cassandra.reaper.io\u0026#34;] keyspace: reaper_db jmxAddressTranslator: type: multiIpPerNode ipTranslations: - from: \u0026#34;10.10.10.111\u0026#34; to: \u0026#34;node1.cassandra.reaper.io\u0026#34; - from: \u0026#34;10.10.10.112\u0026#34; to: \u0026#34;node2.cassandra.reaper.io\u0026#34; When running multi region clusters in AWS, set type to ec2MultiRegion in order to use the EC2MultiRegionAddressTranslator from the Datastax Java Driver.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/configuration/reaper_specific/",
	"title": "Reaper Specific Settings",
	"tags": [],
	"description": "",
	"content": "Configuration settings in the cassandra-reaper.yaml that are specific to Reaper\nautoScheduling Optional setting to automatically setup repair schedules for all non-system keyspaces in a cluster. If enabled, adding a new cluster will automatically setup a schedule repair for each keyspace. Cluster keyspaces are monitored based on a configurable frequency, so that adding or removing a keyspace will result in adding / removing the corresponding scheduled repairs.\nautoScheduling: enabled: true initialDelayPeriod: PT15S periodBetweenPolls: PT10M timeBeforeFirstSchedule: PT5M scheduleSpreadPeriod: PT6H adaptive: true incremental: false percentUnrepairedThreshold: 10 excludedKeyspaces: [myTTLKeyspace, ...] excludedClusters: [myCluster, ...] Definitions for the above sub-settings are as follows.\nenabled Type: Boolean\nDefault: false\nEnables or disables autoScheduling.\ninitialDelayPeriod Type: String\nDefault: PT15S (15 seconds)\nThe amount of delay time before the schedule period starts.\nperiodBetweenPolls Type: String\nDefault: PT10M (10 minutes)\nThe interval time to wait before checking whether to start a repair task.\ntimeBeforeFirstSchedule Type: String\nDefault: PT5M (5 minutes)\nGrace period before the first repair in the schedule is started.\nscheduleSpreadPeriod Type: String\nDefault: PT6H (6 hours)\nThe time spacing between each of the repair schedules that is to be carried out.\nadaptive Type: Boolean\nDefault: true\nWhen enabled, the auto-scheduling will automatically adapt repair schedules based on the cluster\u0026rsquo;s repair history and current state.\nincremental Type: Boolean\nDefault: false\nWhen enabled, auto-scheduled repairs will use incremental repair by default. Note that this is only supported with the PARALLEL repairParallelism setting.\npercentUnrepairedThreshold Type: Integer\nDefault: 10\nThe percentage threshold of unrepaired data that triggers auto-scheduling to create repair schedules. If a keyspace has more than this percentage of unrepaired data, it will be included in auto-scheduling.\nexcludedKeyspaces Type: Array (comma separated Strings)\nThe Keyspaces that are to be excluded from the repair schedule.\nexcludedClusters Type: Array (comma separated Strings)\nThe Clusters that are to be excluded from the repair schedule.\ndatacenterAvailability Type: String\nDefault: ALL\nIndicates to Reaper its deployment in relation to cluster data center network locality. The value must be either ALL, LOCAL, EACH or SIDECAR. Note that this setting controls the behavior for metrics collection.\nBy default, Apache Cassandra restricts JMX communications to localhost only. In this setup, only the SIDECAR value is suitable.\nFor security reasons, it is possible that Reaper will have access limited to nodes in a single datacenter via JMX (multi region clusters for example). In this case, it is possible to deploy an operate an instance of Reaper in each datacenter where each instance only has access via JMX (with or without authentication) to the nodes in its local datacenter. In addition, Reaper will check the number of pending compactions and actively running repairs on all replicas prior to processing a segment.\nALL - requires Reaper to have access via JMX to all nodes across all datacenters. In this mode Reaper can be backed by all available storage types.\nLOCAL - requires Reaper to have access via JMX to all nodes only in the same datacenter local to Reaper. A single Reaper instance can operate in this mode and trigger repairs from within its local data center. In this case, can be backed by all available storage types and repairs to any remote datacenters are be handled internally by Cassandra. A Reaper instance can be deployed to each datacenter and be configured to operate in this mode. In this case, Reaper can use Apache Cassandra as its storage.\nFurther information can be found in the Operating with a Multi DC Cluster section.\nEACH - requires a minimum of one Reaper instance operating in each datacenter. Each Reaper instance is required to have access via JMX to all nodes only in its local datacenter. When operating in this mode, Reaper can use either of Apache Cassandra as its storage. In addition, metrics from nodes in remote datacenters must be collected through the storage backend. If any metric is unavailable, the segment will be postponed for later processing.\nFurther information can be found in the Operating with a Multi DC Cluster section.\nSIDECAR - requires one reaper instance for each node in the cluster. Each Reaper instance is required to have access via JMX to its local node. When operating in this mode, Reaper can use either of Apache Cassandra or Astra as its storage.\nFurther information can be found in the Sidecar Mode section.\nenableCrossOrigin Type: Boolean\nDefault: true\nOptional setting which can be used to enable the CORS headers for running an external GUI application, like this project. When enabled it will allow REST requests incoming from other origins than the domain that hosts Reaper.\nenableDynamicSeedList Type: Boolean\nDefault: true\nAllow Reaper to add all nodes in the cluster as contact points when adding a new cluster, instead of just adding the provided node.\nhangingRepairTimeoutMins Type: Integer\nThe amount of time in minutes to wait for a single repair to finish. If this timeout is reached, the repair segment in question will be cancelled, if possible, and then scheduled for later repair again within the same repair run process.\nhttpManagement Settings to configure HTTP management interface for integration with management APIs like DataStax OpsCenter or similar tools.\nhttpManagement: enabled: false mgmtApiMetricsPort: 9000 keystore: /path/to/keystore.jks truststore: /path/to/truststore.jks truststoresDir: /path/to/truststores enabled Type: Boolean\nDefault: false\nEnables or disables the HTTP management interface.\nmgmtApiMetricsPort Type: Integer\nDefault: 9000\nThe port number for the management API metrics endpoint.\nkeystore Type: String\nPath to the keystore file for SSL/TLS configuration of the management interface.\ntruststore Type: String\nPath to the truststore file for SSL/TLS configuration of the management interface.\ntruststoresDir Type: String\nDirectory path containing truststore files for SSL/TLS configuration.\nincrementalRepair Type: Boolean\nDefault: false\nSets the default repair type unless specifically defined for each run. Note that this is only supported with the PARALLEL repairParallelism setting. For more details in incremental repair, please refer to the following article.http://www.datastax.com/dev/blog/more-efficient-repairs\nNote: It is recommended to avoid using incremental repair before Cassandra 4.0 as subtle bugs can lead to overstreaming and cluster instabililty.\nsubrangeIncrementalRepair Type: Boolean\nDefault: false\nSets the default repair type unless specifically defined for each run. Note that this is only supported with the PARALLEL repairParallelism setting. For more details in incremental repair, please refer to the following article.http://www.datastax.com/dev/blog/more-efficient-repairs. This mode will split the repair jobs into sets of token ranges using the incremental mode. This will prevail over the incrementalRepair setting.\nNote: Subrange incremental repair is only available since Cassandra 4.0.\nblacklistTwcsTables Type: Boolean\nDefault: false\nDisables repairs of any tables that use either the TimeWindowCompactionStrategy or DateTieredCompactionStrategy. This automatic blacklisting is not stored in schedules or repairs. It is applied when repairs are triggered and visible in the UI for running repairs. Not storing which tables are TWCS/DTCS ensures changes to a table\u0026rsquo;s compaction strategy are honored on every new repair.\nNote: It is recommended to enable this option as repairing these tables, when they contain TTL\u0026rsquo;d data, causes overlaps between partitions across the configured time windows the sstables reside in. This leads to an increased disk usage as the older sstables are unable to be expired despite only containing TTL\u0026rsquo;s data. Repairing DTCS tables has additional issues and is generally not recommended.\njmxAuth Optional setting to allow Reaper to establish JMX connections to Cassandra clusters using password based JMX authentication.\njmxAuth: username: cassandra password: cassandra #### `username` username Type: String\nCassandra JMX username.\npassword Type: String\nCassandra JMX password.\njmxCredentials Since 1.1.0 Optional setting to allow Reaper to establish JMX connections to Cassandra clusters with specific credentials per cluster.\njmxCredentials: clusterProduction1: username: user1 password: password1 clusterProduction2: username: user2 password: password2 This setting can be used in conjunction with the jmxAuth to override the credentials for specific clusters only. The cluster name must match the one defined in the cassandra.yaml file (in the example above, clusterProduction1 and clusterProduction2).\nAdding a new cluster with specific credentials requires to add the seed node in the following format : host@cluster To match the example above, it could be something like : 10.0.10.5@clusterProduction1\nWhen passed in as an environment variable, Reaper expects a comma-separated list of user:password@cluster.\njmxConnectionTimeoutInSeconds Type: Integer\nDefault: 20\nControls the timeout for establishing JMX connections. The value should be low enough to avoid stalling simple operations in multi region clusters, but high enough to allow connections under normal conditions.\njmxPorts Type: Object\nOptional mapping of custom JMX ports to use for individual hosts. The used default JMX port value is 7199. CCM users will find IP and port number to add in ~/.ccm/\u0026lt;cluster\u0026gt;/*/node.conf or by running ccm \u0026lt;node\u0026gt; show.\njmxPorts: 127.0.0.1: 7100 127.0.0.2: 7200 127.0.0.3: 7300 logging Settings to configure Reaper logging.\nlogging: level: INFO loggers: io.dropwizard: WARN org.eclipse.jetty: WARN appenders: - type: console logFormat: \u0026quot;%-6level [%d] [%t] %logger{5} - %msg %n\u0026quot; threshold: WARN - type: file logFormat: \u0026quot;%-6level [%t] %logger{5} - %msg %n\u0026quot; currentLogFilename: /var/log/cassandra-reaper/reaper.log archivedLogFilenamePattern: /var/log/cassandra-reaper/reaper-%d.log.gz archivedFileCount: 99 Definitions for some of the above sub-settings are as follows.\nlevel Type: String\nGlobal log level to filter to. Where the level order is ALL \u0026lt; DEBUG \u0026lt; INFO \u0026lt; WARN \u0026lt; ERROR \u0026lt; FATAL \u0026lt; OFF. See the log4j documentation for further information.\nloggers Type: Object\nKey value pair containing the logger class name as the key and other sub-settings as its value.\nlogFormat Type: String\nThe output format of an entry in the log.\nthreshold Type: String\nThe log level to filter the console messages to. Where the level order is ALL \u0026lt; DEBUG \u0026lt; INFO \u0026lt; WARN \u0026lt; ERROR \u0026lt; FATAL \u0026lt; OFF. See the log4j documentation for further information.\narchivedFileCount Type: Integer\nThe number of archive log files stored in the log rotation sliding window. That is the number of archived (compressed) log files kept at one point in time. If there are archivedFileCount number of archived log files and the current (uncompressed) log file is archived, the oldest archived log file is deleted.\nmetrics Type: Object\nConfiguration parameters for sending metrics to a reporting system via the Dropwizard interface. Reaper for Apache Cassandra ships the Graphite and DataDog reporters by default.\nFurther information on metrics configuration can be found in the Metrics section.\nrepairIntensity Type: Float (value between 0.0 and 1.0, but must never be 0.0.)\nRepair intensity defines the amount of time to sleep between triggering each repair segment while running a repair run. When intensity is 1.0, it means that Reaper doesn\u0026rsquo;t sleep at all before triggering next segment, and otherwise the sleep time is defined by how much time it took to repair the last segment divided by the intensity value. 0.5 means half of the time is spent sleeping, and half running. Intensity 0.75 means that 25% of the total time is used sleeping and 75% running. This value can also be overwritten per repair run when invoking repairs.\nrepairManagerSchedulingIntervalSeconds Type: Integer\nDefault: 30\nControls the pace at which the Repair Manager will schedule processing of the next segment. Reducing this value from its default value of 30s to a lower value can speed up fast repairs by orders of magnitude.\nrepairParallelism Type: String\nType of parallelism to apply by default to repair runs. The value must be either SEQUENTIAL, PARALLEL, or DATACENTER_AWARE.\nSEQUENTIAL - one replica at a time, validation compaction performed on snapshots\nPARALLEL - all replicas at the same time, no snapshot\nDATACENTER_AWARE - one replica in each DC at the same time, with snapshots. If this value is used in clusters older than 2.0.12, Reaper will fall back into using SEQUENTIAL for those clusters.\nrepairRunThreadCount Type: Integer\nThe amount of threads to use for handling the Reaper tasks. Have this big enough not to cause blocking in cause some thread is waiting for I/O, like calling a Cassandra cluster through JMX.\nscheduleDaysBetween Type: Integer\nDefault: 7\nDefines the amount of days to wait between scheduling new repairs. The value configured here is the default for new repair schedules, but you can also define it separately for each new schedule. Using value 0 for continuous repairs is also supported.\nscheduleRetryOnError Type: Boolean\nDefault: false\nWhen enabled, repair schedules will be automatically retried if they fail due to errors. This can help with recovering from temporary failures or network issues.\nscheduleRetryDelay Type: String (ISO 8601 Duration)\nDefault: PT1H (1 hour)\nThe delay period before retrying a failed repair schedule. This setting only takes effect when scheduleRetryOnError is enabled.\nsegmentCountPerNode Type: Integer\nDefault: 64\nDefines the default amount of repair segments to create for newly registered Cassandra repair runs, for each node in the cluster. When running a repair run by Reaper, each segment is repaired separately by the Reaper process, until all the segments in a token ring are repaired. The count might be slightly off the defined value, as clusters residing in multiple data centers require additional small token ranges in addition to the expected. This value can be overwritten when executing a repair run via Reaper.\nIn a 10 nodes cluster, setting a value of 20 segments per node will generate a repair run that splits the work into 200 token subranges. This number can vary due to vnodes (before 1.2.0, Reaper cannot create a segment with multiple token ranges, so the number of segments will be at least the total number of vnodes in the cluster). As Reaper tries to size segments evenly, the presence of very small token ranges can lead to have more segments than expected.\npurgeRecordsAfterInDays Type: Integer\nDefault: 30\nDefines the amount of days after which a repair run will get purged from storage.\nserver Settings to configure the application UI server.\nserver: type: default applicationConnectors: - type: http port: 8080 bindHost: 0.0.0.0 adminConnectors: - type: http port: 8081 bindHost: 0.0.0.0 requestLog: appenders: [] Reaper provides two separate UIs; the application UI which is configured via the applicationConnectors settings and the administration UI which is configured via the adminConnectors settings. The application UI provides functionality to create/manage cluster schedules, and the administration UI provides tools for monitoring and debugging the Reaper system.\nport For the applicationConnectors this setting will be the port number used to access the application UI. For the adminConnectors this setting will be the port number to access the administration UI.\nNote that the port numbers for each must be different values when bound to the same host.\nbindHost For the applicationConnectors this setting will be the host address used to access the application UI. For the adminConnectors this setting will be the host address used to access the administration UI.\nNote that to bind the service to all interfaces use value 0.0.0.0 or leave the value for the setting this blank. A value of * is an invalid value for this setting.\nstorageType Type: String\nThe storage type to use in which Reaper will store its control data. The value must be either cassandra, astra or memory. If the recommended (persistent) storage type cassandra, or astra is being used, the database client parameters must be specified in the cassandra section in the configuration file. See the example settings in provided the src/packaging/resources directory of the repository.\nuseAddressTranslator Type: Boolean\nDefault: false\nWhen running multi region clusters in AWS, turn this setting to true in order to use the EC2MultiRegionAddressTranslator from the Datastax Java Driver. This will allow translating the public address that the nodes broadcast to the private IP address that is used to expose JMX.\njmxAddressTranslator Since 2.1.0\nSometimes it\u0026rsquo;s not possible for Cassandra nodes to broadcast addresses that will work for each and every client; for instance, they might broadcast private IPs because most clients are in the same network, but a particular client could be on another network and go through a router. For such cases, you can configure a custom address translator that will perform additional address translation based on configured mapping.\njmxAddressTranslator: type: multiIpPerNode ipTranslations: - from: \u0026#34;10.10.10.111\u0026#34; to: \u0026#34;node1.cassandra.reaper.io\u0026#34; - from: \u0026#34;10.10.10.112\u0026#34; to: \u0026#34;node2.cassandra.reaper.io\u0026#34; When running multi region clusters in AWS, set type to ec2MultiRegion in order to use the EC2MultiRegionAddressTranslator from the Datastax Java Driver. This will allow translating the public address that the nodes broadcast to the private IP address that is used to expose JMX.\naccessControl Settings to activate and configure authentication for the web UI and REST API. Deleting or commenting that block from the yaml file will turn off authentication.\naccessControl: enabled: true # Enable/disable authentication sessionTimeout: PT10M # Session timeout (ISO 8601 duration) jwt: secret: \u0026#34;your-jwt-secret-key\u0026#34; # JWT signing secret (minimum 256 bits) tokenExpirationTime: PT10M # JWT token expiration users: - username: \u0026#34;admin\u0026#34; password: \u0026#34;admin123\u0026#34; roles: [\u0026#34;operator\u0026#34;] - username: \u0026#34;user\u0026#34; password: \u0026#34;user123\u0026#34; roles: [\u0026#34;user\u0026#34;] For detailed authentication configuration and security considerations, see the Authentication documentation.\nrepairThreadCount Type: Integer\nSince Cassandra 2.2, repairs are multithreaded in order to process several token ranges concurrently and speed up the process. This setting allows to set a default for automatic repair schedules. No more than four threads are allowed by Cassandra.\nmaxPendingCompactions Type: Integer\nDefault: 20\nReaper will prevent repair from overwhelming the cluster when lots of SSTables are streamed, by pausing segment processing if there are more than a specific number of pending compactions. Adjust this setting if you have a lot of tables in the cluster and the total number of pending compactions is usually high.\nmaxParallelRepairs Since 2.2.0\nType: Integer\nDefault: 2\nReaper allows concurrent segments from distinct repair runs running on the same nodes at the same time. In order to limit the repair load, it will only allow a limited number of repair runs to run concurrently (by default, two). Repair runs over the threshold will still start and be in RUNNING state, but their segments will be postposned as long as there are too many repairs being processed.\nSetting this value too high could put a lot of pressure on clusters and negatively impact their performance.\ncryptograph Optional settings to configure how confidential text (ie: passwords) are encyrpted/decrypted.\ncryptograph: type: symmetric systemPropertySecret: SOME_SYSTEM_PROPERTY_KEY type The encryption technique used when encrypting, decrypting, or validating confidential text. Symmetric encryption is the default.\nalgorithm Type: String\nDefault: PBKDF2WithHmacSHA512\nThe optional standard name of the requested secret-key algorithm. See the SecretKeyFactory section in the Java Cryptography Architecture Standard Algorithm Name Documentation for information about standard algorithm names.\ncipher Type: String\nDefault: AES/CBC/PKCS5Padding\nThe optional name of the transformation, e.g., AES/CBC/PKCS5Padding. See the Cipher section in the Java Cryptography Architecture Standard Algorithm Name Documentation for information about standard transformation names.\ncipherType Type: String\nDefault: AES\nThe optional name of the secret-key algorithm to be associated with the given key material. See Appendix A in the Java Cryptography Architecture Reference Guide for information about standard algorithm names.\niterationCount Type: Integer\nDefault: 1024\nThe optional number of times the password is hashed.\nkeyStrength Type: Integer\nDefault: 256\nThe optional length in bits of the derived symmetric key\nsalt Type: String\nDefault: deadbeef\nThe optional salt used for creating the PBEKeySpec\nsystemPropertySecret Type: String\nThe key of a system property that holds the shared secret for the symmetric encryption. If encrypted text is required, then this key and its value need to be defined in the environment before reaper can be started.\nexport SOME_SYSTEM_PROPERTY_KEY=mysharedsecret\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/concepts/",
	"title": "Core Concepts",
	"tags": [],
	"description": "",
	"content": "Segments Reaper splits repair runs in segments. A segment is a subrange of tokens that fits entirely in one of the cluster token ranges. The minimum number of segments for a repair run is the number of token ranges in the cluster. With a 3 nodes cluster using 256 vnodes per node, a repair run will have at least 768 segments. If necessary, each repair can define a higher number of segments than the number of token ranges.\nAs of Reaper 1.2.0 and Apache Cassandra 2.2, token ranges that have the same replicas can be consolidated into a single segment. If the total number of requested segments is lower than the number of vnodes, Reaper will try to group token ranges so that each segment has the appropriate number of tokens. In Cassandra 2.2, one repair session will be started for each subrange of the segment, so the gain will be the reduction of overhead in Reaper. Starting with 3.0, Cassandra will generate a single repair session for all the subranges that share the same replicas, which then further reduces the overhead of vnodes in Cassandra.\nBack-pressure Reaper will associate each segment with its replicas, and run repairs sequentially on only one of the replicas. If a repair is already running on one of the replicas or if there are more than 20 pending compactions, Reaper will postpone the segment for future processing and try to repair the next segment.\nConcurrency and Multithreading Being a multi threaded service, Reaper will compute how many concurrent repair sessions can run on the cluster and adjust its thread pool accordingly. To that end, it will check the number of nodes in the cluster and the RF (Replication Factor) of the repaired keyspace. On a three node cluster with RF=3, only one segment can be repaired at a time. On a six node cluster with RF=3, two segments can be repaired at the same time.\nIn the case of a cluster spread around multiple datacenters, the number of concurrent repair will depend on the datacenter with the smallest RF by node. On a cluster composed of a main datacenter with 9 nodes and RF=3 and a backup datacenter with 4 nodes and RF=3, only one segment can be repaired at the same time because of the backup datacenter.\nThe maximum number of concurrent repairs is 15 by default and can be modified in the YAML configuration (cassandra-reaper.yaml) file.\nSince Cassandra 2.2, repairs are multithreaded in order to process several token ranges concurrently and speed up the process. No more than four threads are authorized by Cassandra. The number of repair threads can be set differently for each repair run/schedule. This setting will be ignored for clusters running an older version of Apache Cassandra.\nTimeout By default, each segment must complete within 30 minutes. Reaper subscribes to the repair service notifications of Cassandra to monitor completion, and if a segment takes more than 30 minutes it gets cancelled and postponed. This means that if a repair job is subject to frequent segment cancellation, it is necessary to either split it up into more segments or raise the timeout over its default value.\nPause and Resume Pausing a repair will force the termination of all running segments. Once the job is resumed, cancelled segments will be fully processed once again from the beginning.\nIntensity Intensity controls the eagerness by which Reaper triggers repair segments. The Reaper will use the duration of the previous repair segment to compute how much time to wait before triggering the next one. The idea behind this is that long segments mean a lot of data mismatch, and thus a lot of streaming and compaction. Intensity allows reaper to adequately back off and give the cluster time to handle the load caused by the repair.\nScheduling Interval Reaper polls for new segments to process at a fixed interval. By default the interval is set to 30 seconds and this value can be modified in the Reaper configuration YAML file.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/download/",
	"title": "Downloads",
	"tags": [],
	"description": "",
	"content": "Downloads and Installation Packages The current stable version can be downloaded in the following packaging formats :\nTarball / Debian / RPM The current development version can be downloaded as Debian/RPM packages in the cloudsmith.io beta repository.\nQuick Installation Guide For a docker image, please see the Docker section.\nOnce the appropriate package has been downloaded, head over to the Install and Run section.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/configuration/backend_specific/",
	"title": "Backend Specific Settings",
	"tags": [],
	"description": "",
	"content": "Backend Specific Settings Configuration settings in the cassandra-reaper.yaml that are specific to a particular backend.\nCassandra Settings The following settings are specific to a Reaper deployment that is backed by an Apache Cassandra database. Note that Cassandra backend configuration relies on the Dropwizard-Cassandra module.\nactivateQueryLogger Type: Boolean\nDefault: false\nRecords the CQL calls made to the Cassandra backend in the log output.\ncassandra Settings to configure Reaper to use Cassandra for storage of its control data. Reaper uses the Cassandra Java driver version 4.17.0 to perform operations on the cluster. An example of the configuration settings for the driver are as follows.\ncassandra: type: basic sessionName: \u0026#34;test\u0026#34; contactPoints: - host: 127.0.0.1 port: 9042 sessionKeyspaceName: reaper_db loadBalancingPolicy: type: default localDataCenter: dc1 retryPolicy: type: default schemaOptions: agreementIntervalMilliseconds: 2000 agreementTimeoutSeconds: 10 agreementWarnOnFailure: true requestOptionsFactory: requestTimeout: 20s requestDefaultIdempotence: true authProvider: type: plain-text username: cassandra password: cassandra Definitions for some of the above sub-settings are as follows.\nsessionName Type: String\nName of the session to create\ncontactPoints Type: Array\nSeed nodes in the Cassandra cluster to contact, with their port each. Can be provided as a comma separated list of objects:\n{\u0026#34;host\u0026#34;: \u0026#34;host1\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9042\u0026#34;}, {\u0026#34;host\u0026#34;: \u0026#34;host2\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;9042\u0026#34;} or as a list of objects:\n- host: host1 port: 9042 - host: host2 port: 9042 sessionKeyspaceName Type: String\nName of the keyspace to store the Reaper control data.\nloadBalancingPolicy Settings to configure the policies used to generate the query plan which determines the nodes to connect to when performing query operations. Further information can be found in the Cassandra Java driver Load balancing section.\ntype Type: String\nThe policy type used to contribute to the computation of the query plan.\nlocalDataCenter Type: String\nSpecifies the name of the datacenter closest to Reaper when using the dcAwareRoundRobin policy.\nauthProvider If native protocol authentication is enabled on Cassandra, settings configure Reaper to pass credentials to Cassandra when establishing a connection.\nusername Type: String\nCassandra native protocol username.\npassword Type: String\nCassandra native protocol password.\nrequestTimeout Type: Duration Default: 10s\nThe timeout for requests to the Cassandra cluster.\nFull Configuration Reference See here for more information.\ncassandra: type: basic sessionName: name sessionKeyspaceName: keyspace requestOptionsFactory: requestTimeout: 5s requestConsistency: local requestPageSize: 12 requestSerialConsistency: local requestDefaultIdempotence: true metricsEnabled: true protocolVersion: type: default version: V5 ssl: type: default cipherSuites: [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;] hostValidation: true keyStorePassword: keyStorePassword keyStorePath: keyStorePath trustStorePassword: trustStorePassword trustStorePath: trustStorePath compression: lz4 contactPoints: - host: localhost port: 9041 authProvider: type: plain-text username: admin password: hunter2 retryPolicy: type: default speculativeExecutionPolicy: type: constant delay: 1s maxSpeculativeExecutions: 3 poolingOptions: maxRequestsPerConnection: 5 maxRemoteConnections: 10 maxLocalConnections: 20 heartbeatInterval: 5s connectionConnectTimeout: 10s addressTranslator: type: ec2-multi-region timestampGenerator: type: atomic reconnectionPolicyFactory: type: exponential baseConnectionDelay: 10s maxReconnectionDelay: 30s loadBalancingPolicy: type: default localDataCenter: local dataCenterFailoverAllowLocalConsistencyLevels: true slowAvoidance: true dcFailoverMaxNodesPerRemoteDc: 2 cassandraOptions: # to add options which are not supported by default. Full list can be found at https://docs.datastax.com/en/developer/java-driver/4.11/manual/core/ - type: long name: advanced.protocol.max-frame-length value: 12 sessionMetrics: - continuous-cql-requests nodeMetrics: - bytes-sent schema: agreementIntervalMilliseconds: 200 agreementTimeoutSeconds: 10 agreementWarnOnFailure: true H2 or Postgres Database Settings Removed in v3.0.0\nThe following settings are specific to a Reaper deployment that is backed by either a H2 or Postgres database. An example of the configuration settings for a Postgres database are as follows.\npostgres: url: jdbc:postgresql://127.0.0.1/reaper user: postgres password: Definitions for the above sub-settings are as follows.\nh2 Settings to configure Reaper to use H2 for storage of its control data.\npostgres Settings to configure Reaper to use Postgres for storage of its control data.\ndriverClass Type: String\nWARNING this setting is DEPRECATED and its usage should be avoided.\nSpecifies the driver to use to connect to the database.\nurl Type: String\nSpecifies the URL to connect to the database (either H2 or Postgres) on.\nuser Type: String\nDatabase username.\npassword Type: String\nDatabase password.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/backends/",
	"title": "Backends",
	"tags": [],
	"description": "",
	"content": "Reaper for Apache Cassandra can be used with either an ephemeral memory storage or persistent database. For persistent scalable database storage, a Cassandra cluster can be set up to back Reaper. To use a Cassandra cluster as the backed storage for Reaper set storageType to a value of cassandra in the Reaper configuration file.\nFurther information on the available storage options is provided in the following section.\nIn-Memory Cassanda Sample YAML files are available in the src/packaging/resource directory for each of the above storage options:\ncassandra-reaper-memory.yaml cassandra-reaper-cassandra.yaml For configuring other aspects of the service, see the available configuration options in the Configuration Reference.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/usage/health/",
	"title": "Checking a Cluster&#39;s Health",
	"tags": [],
	"description": "",
	"content": "Dashboard When a cluster has been added to Reaper it will be displayed in the dashboard.\nNode View Clicking on one of the nodes will open a dialog box containing details of the node\u0026rsquo;s state.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/configuration/docker_vars/",
	"title": "Docker Variables",
	"tags": [],
	"description": "",
	"content": "The Reaper Docker container has been designed to be highly configurable. Many of the environment variables map directly or indirectly to a settings in the cassandra-reaper.yaml configuration file.\nDirect Mapping to Reaper Specific Configuration Settings The Docker environment variables listed in this section map directly to Reaper specific settings in the cassandra-reaper.yaml configuration file. The following table below lists the Docker environment variables, their associated Reaper specific setting in the cassandra-reaper.yaml configuration file, and the default value assigned by the Docker container (if any). Definitions for each Docker environment variable can be found via the link to the associated setting.\nEnvironment Variable Configuration Setting Default Value REAPER_AUTO_SCHEDULING_ENABLED enabled false REAPER_AUTO_SCHEDULING_EXCLUDED_KEYSPACES excludedKeyspaces [] REAPER_AUTO_SCHEDULING_EXCLUDED_CLUSTERS excludedClusters [] REAPER_AUTO_SCHEDULING_INITIAL_DELAY_PERIOD initialDelayPeriod PT15S REAPER_AUTO_SCHEDULING_PERIOD_BETWEEN_POLLS periodBetweenPolls PT10M REAPER_AUTO_SCHEDULING_SCHEDULE_SPREAD_PERIOD scheduleSpreadPeriod PT6H REAPER_AUTO_SCHEDULING_TIME_BEFORE_FIRST_SCHEDULE timeBeforeFirstSchedule PT5M REAPER_AUTO_SCHEDULING_ADAPTIVE adaptive true REAPER_AUTO_SCHEDULING_INCREMENTAL incremental false REAPER_AUTO_SCHEDULING_PERCENT_UNREPAIRED_THRESHOLD percentUnrepairedThreshold 10 REAPER_BLACKLIST_TWCS blacklistTwcsTables false REAPER_DATACENTER_AVAILABILITY datacenterAvailability ALL REAPER_ENABLE_CROSS_ORIGIN enableCrossOrigin true REAPER_ENABLE_DYNAMIC_SEED_LIST enableDynamicSeedList true REAPER_HANGING_REPAIR_TIMEOUT_MINS hangingRepairTimeoutMins 30 REAPER_INCREMENTAL_REPAIR incrementalRepair false REAPER_SUBRANGE_INCREMENTAL subrangeIncrementalRepair false REAPER_JMX_AUTH_PASSWORD password REAPER_JMX_AUTH_USERNAME username REAPER_JMX_CREDENTIALS jmxCredentials REAPER_JMX_CONNECTION_TIMEOUT_IN_SECONDS jmxConnectionTimeoutInSeconds 20 REAPER_JMX_PORTS jmxPorts {} REAPER_LOGGING_APPENDERS_CONSOLE_LOG_FORMAT logFormat \u0026ldquo;%-6level [%d] [%t] %logger{5} - %msg %n\u0026rdquo; REAPER_LOGGING_APPENDERS_CONSOLE_THRESHOLD threshold INFO REAPER_LOGGING_LOGGERS loggers {} REAPER_LOGGING_ROOT_LEVEL level INFO REAPER_MAX_PENDING_COMPACTIONS maxPendingCompactions 20 REAPER_METRICS_FREQUENCY fequency 1 minute REAPER_METRICS_REPORTERS reporters [] REAPER_REPAIR_INTENSITY repairIntensity 0.9 REAPER_REPAIR_MANAGER_SCHEDULING_INTERVAL_SECONDS repairManagerSchedulingIntervalSeconds 30 REAPER_REPAIR_PARALELLISM repairParallelism DATACENTER_AWARE REAPER_REPAIR_RUN_THREADS repairRunThreadCount 15 REAPER_SCHEDULE_DAYS_BETWEEN scheduleDaysBetween 7 REAPER_SEGMENT_COUNT_PER_NODE segmentCountPerNode 64 REAPER_SERVER_ADMIN_BIND_HOST bindHost 0.0.0.0 REAPER_SERVER_ADMIN_PORT port 8081 REAPER_SERVER_APP_BIND_HOST bindHost 0.0.0.0 REAPER_SERVER_APP_PORT port 8080 REAPER_STORAGE_TYPE storageType memory REAPER_USE_ADDRESS_TRANSLATOR useAddressTranslator false REAPER_MAX_PARALLEL_REPAIRS maxParallelRepairs 2 CRYPTO_SYSTEM_PROPERTY_SECRET cryptograph/systemPropertySecret Unset REAPER_HTTP_MANAGEMENT_ENABLE httpManagement/enabled false REAPER_HTTP_MANAGEMENT_KEYSTORE_PATH httpManagement/keystorePath REAPER_HTTP_MANAGEMENT_TRUSTSTORE_PATH httpManagement/truststorePath REAPER_HTTP_MANAGEMENT_TRUSTSTORES_DIR httpManagement/truststoresDir REAPER_MGMT_API_METRICS_PORT mgmtApiMetricsPort 9000 REAPER_PURGE_RECORDS_AFTER_IN_DAYS purgeRecordsAfterInDays 30 Runtime Configuration Variables The following Docker environment variables control runtime behavior and do not map directly to configuration file settings:\nEnvironment Variable Description Default Value REAPER_HEAP_SIZE JVM heap size for the Reaper process 1G REAPER_TMP_DIRECTORY Temporary directory for Reaper operations /var/tmp/cassandra-reaper REAPER_MEMORY_STORAGE_DIRECTORY Directory for memory storage persistence /var/lib/cassandra-reaper/storage JAVA_OPTS Additional JVM options to pass to the Reaper process Cluster Registration Variables The following Docker environment variables are used when running the register-clusters command:\nEnvironment Variable Description Default Value REAPER_AUTO_SCHEDULING_SEEDS Comma-separated list of \u0026lsquo;host:port\u0026rsquo; entries for Cassandra nodes REAPER_HOST Hostname of the Reaper instance REAPER_PORT Port of the Reaper instance 8080 Using Address translator mapping The Docker environment variables listed in this section are those related to the feature address translator mapping.\nEnvironment Variable Configuration Setting Example Values REAPER_CASS_ADDRESS_TRANSLATOR_TYPE addressTranslator ec2MultiRegion or multiIpPerNode REAPER_CASS_ADDRESS_TRANSLATOR_MAPPING addressTranslator host1:ip1,host2:ip2,host3:ip3 JMX_ADDRESS_TRANSLATOR_TYPE jmxAddressTranslator ec2MultiRegion or multiIpPerNode JMX_ADDRESS_TRANSLATOR_MAPPING jmxAddressTranslator host1:ip1,host2:ip2,host3:ip3 Example :\nREAPER_CASS_ADDRESS_TRANSLATOR_TYPE=multiIpPerNode REAPER_CASS_ADDRESS_TRANSLATOR_MAPPING=host1:ip1,host2:ip2 config bloc at the container startup file \u0026lsquo;/etc/cassandra-reaper/cassandra-reaper.yml\u0026rsquo; :\naddressTranslator: type: multiIpPerNode ipTranslations: - from: \u0026#34;host1\u0026#34; to: \u0026#34;ip1\u0026#34; - from: \u0026#34;host2\u0026#34; to: \u0026#34;ip2\u0026#34; and the same thing for the jmx mapping\nJMX_ADDRESS_TRANSLATOR_TYPE=multiIpPerNode JMX_ADDRESS_TRANSLATOR_MAPPING=host1:ip1,host2:ip2 result\njmxAddressTranslator: type: multiIpPerNode ipTranslations: - from: \u0026#34;host1\u0026#34; to: \u0026#34;ip1\u0026#34; - from: \u0026#34;host2\u0026#34; to: \u0026#34;ip2\u0026#34; Note:\nAuthentication Configuration Variables The following Docker environment variables control authentication settings. These variables are handled specially by configuration scripts that run at container startup.\nEnvironment Variable Description Required REAPER_AUTH_ENABLED Enable/disable authentication globally No (default: true) REAPER_AUTH_USER Username for the admin user with operator role Yes (when auth is enabled) REAPER_AUTH_PASSWORD Password for the admin user Yes (when auth is enabled) REAPER_READ_USER Username for optional read-only user No (only configured if both user and password are set) REAPER_READ_USER_PASSWORD Password for optional read-only user No (only configured if both user and password are set) Note: The read-only user (REAPER_READ_USER and REAPER_READ_USER_PASSWORD) is configured conditionally. The user is only added to the authentication configuration if both environment variables are set to non-empty values. This prevents environment variable resolution errors while maintaining security.\nAssociated Reaper Specific Configuration Settings The following Docker environment variables have no direct mapping to a setting in the cassandra-reaper.yaml configuration file. However, they do affect the content contained in the file that is Reaper specific.\nREAPER_METRICS_ENABLED Type: Boolean\nDefault: false\nAllows the sending of Reaper metrics to a metrics reporting system such as Graphite. If enabled, the other associated environment variables REAPER_METRICS_FREQUENCY and REAPER_METRICS_REPORTERS must be set to appropriate values in order for metrics reporting to function correctly.\nMetrics reporter definition syntax REAPER_METRICS_REPORTERS Type: List\nDefault: []\nDefines the metrics reporters, using a JSON syntax instead of a YAML one. To activate graphite metrics reporting, run the container with the following sample arguments :\ndocker run \\ -p 8080:8080 \\ -p 8081:8081 \\ -e \u0026#34;REAPER_METRICS_ENABLED=true\u0026#34; \\ -e \u0026#34;REAPER_METRICS_FREQUENCY=10 second\u0026#34; \\ -e \u0026#34;REAPER_METRICS_REPORTERS=[{type: graphite, host: my.graphite.host.com, port: 2003, prefix: my.prefix}]\u0026#34; \\ thelastpickle/cassandra-reaper:master Direct Mapping to Cassandra Backend Specific Configuration Settings The Docker environment variables listed in this section map directly to Cassandra backend specific settings in the cassandra-reaper.yaml configuration file. The following table below lists the Docker environment variables, their associated Cassandra backend specific setting in the cassandra-reaper.yaml configuration file, and the default value assigned by the Docker container (if any). Definitions for each Docker environment variable can be found via the link to the associated setting.\nIn order for the Cassandra backend to be used, REAPER_STORAGE_TYPE must be set to cassandra.\nEnvironment Variable Configuration Setting Default Value REAPER_CASS_ACTIVATE_QUERY_LOGGER activateQueryLogger false REAPER_CASS_CLUSTER_NAME clusterName clustername REAPER_CASS_CONTACT_POINTS contactPoints {\u0026ldquo;host\u0026rdquo;: \u0026ldquo;127.0.0.1\u0026rdquo;, \u0026ldquo;port\u0026rdquo;: \u0026ldquo;9042\u0026rdquo;}, {\u0026ldquo;host\u0026rdquo;: \u0026ldquo;127.0.0.2\u0026rdquo;, \u0026ldquo;port\u0026rdquo;: \u0026ldquo;9042\u0026rdquo;} REAPER_CASS_KEYSPACE keyspace reaper_db REAPER_CASS_LOCAL_DC localDC REAPER_CASS_AUTH_USERNAME username cassandra REAPER_CASS_AUTH_PASSWORD password cassandra REAPER_CASS_SCHEMA_AGREEMENT_INTERVAL agreementIntervalMilliseconds 2000 REAPER_CASS_SCHEMA_AGREEMENT_TIMEOUT agreementTimeoutSeconds 10 REAPER_CASS_REQUEST_TIMEOUT requestTimeout 10s Note:\nSome variable names and defaults have changed between the release of Docker-support and Reaper for Apache Cassandra 1.0. The following Cassandra Backend specific variable name changes have occurred in an effort to match closely with our YAML parameter names:\nPre Reaper 1.0 Post Reaper 1.0 REAPER_ACTIVATE_QUERY_LOGGER REAPER_CASS_ACTIVATE_QUERY_LOGGER The following default values have changed:\nEnvironment Variable Previous Default New Default REAPER_CASS_KEYSPACE cassandra-reaper reaper_db REAPER_CASS_SCHEMA_AGREEMENT_TIMEOUT 2000 10 Associated Cassandra Backend Specific Configuration Settings The following Docker environment variables have no direct mapping to a setting in the cassandra-reaper.yaml configuration file. However, the do affect the content contained in the file that is Cassandra backend specific.\nREAPER_CASS_AUTH_ENABLED Type: Boolean\nDefault: false\nAllows Reaper to send authentication credentials when establishing a connection with Cassandra via the native protocol. When enabled, authentication credentials must be specified by setting values for REAPER_CASS_AUTH_USERNAME and REAPER_CASS_AUTH_PASSWORD.\nREAPER_CASS_ADDRESS_TRANSLATOR_ENABLED Type: Boolean\nDefault: false\nEnables the use of address translation for Cassandra connections. When enabled, REAPER_CASS_ADDRESS_TRANSLATOR_TYPE and REAPER_CASS_ADDRESS_TRANSLATOR_MAPPING must be set appropriately.\nREAPER_CASS_NATIVE_PROTOCOL_SSL_ENCRYPTION_ENABLED Type: Boolean\nDefault: false\nAllows Reaper to establish an encrypted connection when establishing a connection with Cassandra via the native protocol.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/configuration/",
	"title": "Configuration Reference",
	"tags": [],
	"description": "",
	"content": "An example testing configuration YAML file can be found from within this project repository: src/server/src/test/resources/cassandra-reaper.yaml.\nThe configuration file structure is provided by Dropwizard, and help on configuring the server, database connection, or logging, can be found on the Dropwizard Configuration Reference\nThe configuration is broken into the following sections:\nReaper Specific - Provides details on settings specific to Reaper. Backend Specific - Provides details on settings specific to the different backends that can be used with Reaper; Cassandra, H2 and Postgres. Docker Variables - Provides details on the Docker Variables that can be used to configure Reaper. Note that Cassandra backend configuration relies on the Dropwizard-Cassandra module.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/metrics/",
	"title": "Metrics",
	"tags": [],
	"description": "",
	"content": "Reaper metrics are provided via the Dropwizard Metrics interface. The interface gives Reaper the ability to configure various metrics reporting systems. Metrics reporting can be configured in Reaper with the metrics property in the Reaper configuration YAML file. The metrics property has two fields; frequency and reporters. Specific metric reporters are defined in the reporters field as follows.\nmetrics: frequency: 1 minute reporters: - type: \u0026lt;type\u0026gt; "
},
{
	"uri": "http://cassandra-reaper.io/docs/usage/",
	"title": "Using Reaper",
	"tags": [],
	"description": "",
	"content": "Starting Reaper The preferred way of running Reaper is to make it a service, which is the default for debian and RPM packaged installations. It can also be started from the command line for tarball/source installs, in two different ways:\nby invoking src/packaging/bin/cassandra-reaper by running java -jar \u0026lt;path/to/cassandra-reaper-X.X.X.jar\u0026gt; server \u0026lt;path/to/cassandra-reaper.yaml\u0026gt; Schema migrations Schema migrations are executed on startup and assume that the database/keyspace already exists. Reaper can run in schema migration mode only, exiting right after the database schema was upgraded, by running the following command:\njava -jar \u0026lt;path/to/cassandra-reaper-X.X.X.jar\u0026gt; schema-migration \u0026lt;path/to/cassandra-reaper.yaml\u0026gt; Using Reaper This section discusses the normal usage of Reaper on a day to day basis.\nReaper includes a community-driven web interface that can be accessed at:\nhttp://$REAPER_HOST:8080/webui/index.html\nThe web interface provides the ability to:\nAdd/remove clusters Manage repair schedules Run manual repairs and manage running repairs "
},
{
	"uri": "http://cassandra-reaper.io/docs/usage/single/",
	"title": "Running a Cluster Repair",
	"tags": [],
	"description": "",
	"content": "Reaper has the ability to launch a once-off repair on a cluster. This can be done in the following way.\nStart a New Repair Click the repair menu item on the left side to navigate to the Repair page. Click Start a new repair to open the repair details form.\nFill in the Details Enter values for the keyspace, tables, owner and other fields and click the Repair button. See the table below for further information on the details for each field.\nOption | Description ---|--- **Cluster** | This field maps to a cluster as defined in the Cluster managment page. **Keyspace** | Restricts the keyspaces that will be repaired by this task. **Tables** | A comma-delimited list of tables to repair. **Owner** | Any string is accepted for this field which acts as a way to include notes in cases where many users have access to Reaper. **Segments per node** | The number of segments to create per nodes in the cluster. **Parallelism** | Options are: - `Sequential`: Used in cases where data center aware repairs are not yet supported (pre-2.0.12). - `Parallel`: Executes repairs across all nodes in parallel. - `DC-Aware`: Executes repairs across all nodes in a data center, one data center at a time. This is the safest option as it restricts extreme load to a specific data center, rather than impacting the full cluster. **Repair intensity** | A value between 0.0 and 1.0, where 1.0 ensures that no sleeping occurs between repair sessions and 0.5 ensures that equal amounts of time while the repair task is running, will be spent sleeping and repairing. **Cause** | Any string is accepted for this field which acts as a way to include notes in cases where many users have access to Reaper. **Incremental** | This boolean value is only supported when the Parallelism is set to Parallel. **Nodes** | Allows to restrict the repair to token ranges of specific nodes. **Datacenters** | Allows to restrict the repair to specific datacenters. **Repair threads** | (Since Cassandra 2.2) Allows to set a number of threads to process several token ranges concurrently. "
},
{
	"uri": "http://cassandra-reaper.io/docs/usage/schedule/",
	"title": "Scheduling a Cluster Repair",
	"tags": [],
	"description": "",
	"content": "Reaper has the ability to create and manage repair schedules for a cluster. This can be done in the following way.\nSetup a Repair Schedule Click the schedule menu item on the left side to navigate to the Schedules page. Click Add schedule to open the schedule details form.\nFill in the Details Enter values for the keyspace, tables, owner and other fields and click Add Schedule button. The details for adding a schedule are similar to the details for Repair form except the \u0026ldquo;Cause\u0026rdquo; field is replaced with three fields; \u0026ldquo;Start time\u0026rdquo;, \u0026ldquo;Interval in days\u0026rdquo; and \u0026ldquo;Percent unrepaired threshold\u0026rdquo;. See the table below for further information the two fields.\nOption | Description ---|--- **Start time** | The time to trigger repairs, based in GMT. **Interval in days** | The frequency for the schedule to be run. **Percent unrepaired threshold** | *For incremental repair only!* Sets the percentage of unrepaired data over which a repair run will be started for this schedule. As soon as one table in the set of tables managed by this schedule gets over the threshold, the run will be triggered. After creating a scheduled repair, the page is updated with a list of active and paused repair schedules.\nNote that when choosing to add a new repair schedule, it is recommended to restrict the repair schedules to specific tables, instead of scheduling repairs for an entire keyspace. Creating different repair schedules will allow for simpler scheduling, fine-grain tuning for more valuable data, and easily grouping tables with smaller data load into different repair cycles.\nFor example, if there are certain tables that contain valuable data or a business requirement for high consistency and high availability, they could be schedule to be repaired during low traffic periods.\nNote that scheduled repairs can be paused and deleted by users with access to the Reaper web interface. To add authentication security the web UI see the authentication section for further information.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/download/docker/",
	"title": "Docker",
	"tags": [],
	"description": "",
	"content": "Docker Docker and Docker Compose will need to be installed in order to use the commands in this section.\nBuilding Reaper Docker Image Prerequisite The generation of the Docker image requires that the JAR file be built and placed in the src/packages directory. If the JAR package is missing from the directory then it can built using either the steps in the Docker package build section (above), or in the Building from Source section.\nBuilding Image To build the Reaper Docker Image which is then added to the local image cache using the cassandra-reaper:latest tag, run the following command from the top level directory.\nmvn -pl src/server/ docker:build -Ddocker.directory=src/server/src/main/docker Docker Hub Image A prebuilt Docker Image is available for download from Docker Hub. The image TAG can be specified when pulling the image from Docker Hub to pull a particular version. Set:\nTAG=master to run Reaper with the latest commits TAG=latest to run Reaper with the latest stable release To pull the image from Docker Hub with a particular tag, run the following command.\ndocker pull thelastpickle/cassandra-reaper:${TAG} Start Docker Environment Using Docker Reaper can be executed within a Docker container with either an ephemeral memory storage or persistent database.\nIn-Memory Backend To launch a Reaper container backed by an In-Memory backend, use the following example with the appropriate JMX authentication settings for the cluster it will manage repairs for.\nTAG=latest REAPER_JMX_AUTH_USERNAME=reaperUser REAPER_JMX_AUTH_PASSWORD=reaperPass # Authentication credentials (required for security) REAPER_AUTH_USER=admin REAPER_AUTH_PASSWORD=your-secure-admin-password docker run \\ -p 8080:8080 \\ -p 8081:8081 \\ -e \u0026#34;REAPER_JMX_AUTH_USERNAME=${REAPER_JMX_AUTH_USERNAME}\u0026#34; \\ -e \u0026#34;REAPER_JMX_AUTH_PASSWORD=${REAPER_JMX_AUTH_PASSWORD}\u0026#34; \\ -e \u0026#34;REAPER_AUTH_USER=${REAPER_AUTH_USER}\u0026#34; \\ -e \u0026#34;REAPER_AUTH_PASSWORD=${REAPER_AUTH_PASSWORD}\u0026#34; \\ thelastpickle/cassandra-reaper:${TAG} Then visit the the Reaper UI: http://localhost:8080/webui/.\nCassandra Backend To launch a Reaper container backed by Cassandra, use the following example to connect to a Cassandra cluster that already has the reaper_db keyspace. Set the appropriate JMX authentication settings for the cluster that Reaper will manage repairs for.\nTAG=latest REAPER_JMX_AUTH_USERNAME=reaperUser REAPER_JMX_AUTH_PASSWORD=reaperPass # Authentication credentials (required for security) REAPER_AUTH_USER=admin REAPER_AUTH_PASSWORD=your-secure-admin-password REAPER_CASS_CLUSTER_NAME=reaper-cluster REAPER_CASS_CONTACT_POINTS={\\\u0026#34;host\\\u0026#34;: \\\u0026#34;192.168.2.185\\\u0026#34;, \\\u0026#34;port\\\u0026#34;: \\\u0026#34;9042\\\u0026#34;} docker run \\ -p 8080:8080 \\ -p 8081:8081 \\ -e \u0026#34;REAPER_JMX_AUTH_USERNAME=${REAPER_JMX_AUTH_USERNAME}\u0026#34; \\ -e \u0026#34;REAPER_JMX_AUTH_PASSWORD=${REAPER_JMX_AUTH_PASSWORD}\u0026#34; \\ -e \u0026#34;REAPER_AUTH_USER=${REAPER_AUTH_USER}\u0026#34; \\ -e \u0026#34;REAPER_AUTH_PASSWORD=${REAPER_AUTH_PASSWORD}\u0026#34; \\ -e \u0026#34;REAPER_STORAGE_TYPE=cassandra\u0026#34; \\ -e \u0026#34;REAPER_CASS_CLUSTER_NAME=${REAPER_CASS_CLUSTER_NAME}\u0026#34; \\ -e \u0026#34;REAPER_CASS_CONTACT_POINTS=${REAPER_CASS_CONTACT_POINTS}\u0026#34; \\ -e \u0026#34;REAPER_CASS_KEYSPACE=reaper_db\u0026#34; \\ thelastpickle/cassandra-reaper:${TAG} Then visit the the Reaper UI: http://localhost:8080/webui/.\nUsing Docker Compose The Docker Compose services available allow for orchestration of an environment that uses Reaper\u0026rsquo;s default settings. This provides a quick way to start Reaper and become familiar with its usage without the need of additional infrastructure. The environment created using Docker Compose comprises a single containerised Apache Cassandra node and a single containerised Reaper service.\nIn addition to the environment using Reaper\u0026rsquo;s default settings, Docker Compose services are provided that allow orchestration of an environment in which the connections between Reaper and Cassandra are SSL encrypted. The services which create this environment contain a -ssl suffix in their name.\nIt is also possible to automate registering the Cassandra cluster with Reaper and making Reaper start repairing non-system keyspaces automatically. To register a cluster, run the Reaper image with the register-cluster command:\nregister-clusters: image: cassandra-reaper:latest links: - cassandra - reaper-in-memory command: [\u0026#34;register-clusters\u0026#34;, \u0026#34;cassandra:7199\u0026#34;, \u0026#34;reaper-in-memory\u0026#34;, \u0026#34;8080\u0026#34;] The register-clusters arguments are:\ncassandra:7199 - host and (JMX) port of a node in the cluster we want to register. reaper-in-memory - hostname of the Reaper instance. 8080 - the port on which Reaper listens on. To make Reaper automatically start repairs, make sure the following environment variables are set:\nREAPER_AUTO_SCHEDULING_ENABLED=\u0026#34;True\u0026#34; REAPER_AUTO_SCHEDULING_TIME_BEFORE_FIRST_SCHEDULE=\u0026#34;PT1M\u0026#34; REAPER_AUTO_SCHEDULING_PERIOD_BETWEEN_POLLS=\u0026#34;PT1M\u0026#34; All available Docker Compose services can be found in the docker-compose.yml file.\nDefault Settings Environment From the top level directory change to the src/packaging directory:\ncd src/packaging Start the Cassandra cluster:\ndocker-compose up cassandra The nodetool Docker Compose service can be used to check on the Cassandra node\u0026rsquo;s status:\ndocker-compose run nodetool status You can alternatively attach directly to the Cassandra container and run nodetool status from within it by running:\ndocker-compose exec cassandra /bin/bash Now that you have a bash shell in the container, you can run nodetool -u reaperUser -pwf /etc/cassandra/jmxremote.password.\nOnce the Cassandra node is online and accepting CQL connections, create the required reaper_db Cassandra keyspace to allow Reaper to save its cluster and scheduling data.\nBy default, the reaper_db keyspace is created using a replication factor of 1. To change this replication factor, provide the intended replication factor as an optional argument:\ndocker-compose run cqlsh-initialize-reaper_db [$REPLICATION_FACTOR] Wait a few moments for the reaper_db schema change to propagate, then start Reaper:\ndocker-compose up reaper SSL Encrypted Connections Environment From the top level directory change to the src/packaging directory:\ncd src/packaging Generate the SSL Keystore and Truststore which will be used to encrypt the connections between Reaper and Cassandra.\ndocker-compose run generate-ssl-stores Start the Cassandra cluster which encrypts both the JMX and Native Protocol:\ndocker-compose up cassandra-ssl The nodetool-ssl Docker Compose service can be used to check on the Cassandra node\u0026rsquo;s status:\ndocker-compose run nodetool-ssl status Once the Cassandra node is online and accepting encrypted SSL connections via the Native Transport protocol, create the required reaper_db Cassandra keyspace to allow Reaper to save its cluster and scheduling data.\nBy default, the reaper_db keyspace is created using a replication factor of 1. To change this replication factor, provide the intended replication factor as an optional argument:\ndocker-compose run cqlsh-initialize-reaper_db-ssl [$REPLICATION_FACTOR] Wait a few moments for the reaper_db schema change to propagate, then start the Reaper service that will establish encrypted connections to Cassandra:\ndocker-compose up reaper-ssl Access The Environment Once started, the UI can be accessed through:\nhttp://127.0.0.1:8080/webui/\nA nodetool Docker Compose service is included for both the default and SSL encrypted environments to allow nodetool commands to be performed on Cassandra.\nFor the default environment use:\ndocker-compose run nodetool status For the SSL encrypted environment use:\ndocker-compose run nodetool-ssl status When adding the Cassandra node to the Reaper UI with the IP address, the above commands can be used to find the node IP address. You can also add the Cassandra node to the Reaper UI by hostname. The container\u0026rsquo;s hostname is the docker-compose service name.\nA cqlsh Docker Compose service is included as well for both the default and SSL encrypted environments to allow the creation of user tables in Cassandra.\nFor the default environment use:\ndocker-compose run cqlsh For the SSL encrypted environment use:\ndocker-compose run cqlsh-ssl Destroying the Docker Environment When terminating the infrastructure, use the following command to stop all related Docker Compose services:\ndocker-compose down To completely clean up all persistent data, delete the ./data/ directory:\nrm -rf ./data/ "
},
{
	"uri": "http://cassandra-reaper.io/docs/usage/multi_dc_non-distributed/",
	"title": "Multi DCs with One Reaper",
	"tags": [],
	"description": "",
	"content": "Reaper can operate clusters which has a multi datacenter deployment. The datacenterAvailability setting in the Reaper YAML file indicates to Reaper its deployment in relation to cluster data center network locality.\nSingle Reaper instance with JMX accessible for all DCs In the case where the JMX port is accessible (with or without authentication) from the running Reaper instance for all nodes in all DCs, it is possible to have a single instance of Reaper handle one or multiple clusters by using the following setting in the configuration yaml file :\ndatacenterAvailability: ALL This setup works with all backends : Apache Cassandra and Memory.\nReaper must be able to access the JMX port (7199 by default) and port 9042 if the cluster is also used as Cassandra backend, on the local DC.\nThe keyspaces must be replicated using NetworkTopologyStrategy (NTS) and have replicas at least on the DC Reaper can access through JMX. Repairing the remote DC will be handled internally by Cassandra.\nNote : multiple instances of Reaper can be running at once with this setting only when using the Apache Cassandra backend. See distributed mode for more details.\nSingle Reaper instance with JMX accessible for limited DCs In the case where the JMX port is accessible (with or without authentication) from the running Reaper instance for all nodes in only some of the DCs, it is possible to have a single instance of Reaper handle one or multiple clusters by using the following setting in the configuration yaml file :\ndatacenterAvailability: LOCAL Be aware that this setup will not allow to handle backpressure for those remote DCs as JMX metrics (pending compactions, running repairs) from those remote nodes are not made available to Reaper.\nIf multiple clusters are registered in Reaper it is required that Reaper can access all nodes in at least one data center in each of the registered clusters.\nThis setup works with all backends : Apache Cassandra and Memory.\nReaper must be able to access the JMX port (7199 by default) and port 9042 if the cluster is also used as Cassandra backend, on the local DC.\nThe keyspaces must be replicated using NetworkTopologyStrategy (NTS) and have replicas at least on the DC Reaper can access through JMX. Repairing the remote DC will be handled internally by Cassandra.\nNote : multiple instances of Reaper can be running at once with this settings only using when the Apache Cassandra backend. See distributed mode for more details.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/usage/multi_dc_distributed/",
	"title": "Multi DCs with Multi Reapers",
	"tags": [],
	"description": "",
	"content": "Multiple Reaper instances can operate clusters which have multi datacenter deployment. Multiple Reaper instances, also known as Distributed mode, can only be used when using the Apache Cassandra backend. Using multiple Reaper instances allows improved availability and fault tolerance. It is more likely that a Reaper UI is available via one of the Reaper instances, and that scheduled repairs are executed by one of the running Reaper instances.\nThe datacenterAvailability setting in the Reaper YAML file indicates to Reaper its deployment in relation to cluster data center network locality.\nMultiple Reaper instances with JMX accessible for all DCs In the case where the JMX port is accessible (with or without authentication) from the running Reaper instance for all nodes in all DCs, it is possible to have multiple instances of Reaper handle one or multiple clusters by using the following setting in the configuration yaml file :\ndatacenterAvailability: ALL Reaper must be able to access the JMX port (7199 by default) and port 9042 if the cluster is also used as Cassandra backend, on the local DC.\nMultiple Reaper instances with JMX accessible for limited DCs In the case where the JMX port is accessible (with or without authentication) from the running Reaper instance for all nodes in only some of the DCs, it is possible to have multiple instances of Reaper handle one or multiple clusters by using the following setting in the configuration yaml file :\ndatacenterAvailability: LOCAL Note, there is no backpressure for nodes in any datacenter if no Reaper instances have JMX access to that datacenter. This is because the JMX metrics (pending compactions, running repairs) required for backpressure is not available from those remote nodes.\nIf multiple clusters are registered in Reaper it is required that some Reaper instances can access all the nodes in at least one of the datacenters in each of the registered clusters.\nLOCAL mode allows you to register multiple clusters in a distributed Reaper installation. LOCAL mode also allows you to prioritize repairs running according to their schedules over worrying about the load on remote and unaccessible datacenters and nodes.\nReaper must be able to access the JMX port (7199 by default) and port 9042 if the cluster is also used as Cassandra backend, on the local DC.\nAny keyspaces that only have replicas in remote JMX unreachable datacenters can not be repaired by Reaper.\nMultiple Reaper instances with JMX accessible locally to each DC In the case where the JMX port is accessible (with or without authentication) from the running Reaper instance for all nodes in the current DC only, it is possible to have a multiple instances of Reaper running in different DCs by using the following setting in the configuration yaml file:\ndatacenterAvailability: EACH This setup prioritises handling backpressure on all nodes over running repairs. Where latency of requests and availability of nodes takes precedence over scheduled repairs this is the safest setup in Reaper.\nThere must be installed and running a Reaper instance in every datacenter of every registered Cassandra cluster. And every Reaper instance must have CQL access to the backend Cassandra cluster it uses as a backend.\nReaper must be able to access the JMX port (7199 by default) and port 9042 if the cluster is also used as Cassandra backend, on the local DC.\nMultiple Reaper instances with JMX access restricted to localhost By default, Cassandra starts up with JMX access restricted to the local machine. This is considered by many companies as being a security requirement. In this case, one reaper instance must be collocated as a sidecar with each Cassandra process, using the following setting in the configuration yaml file:\ndatacenterAvailability: SIDECAR There must be installed and running a Reaper instance on each Cassandra node in the cluster. And every Reaper instance must have CQL access to the backend Cassandra cluster it uses as a backend.\nReaper must be able to access the JMX port (7199 by default) and port 9042 if the cluster is also used as Cassandra backend, on the local DC.\nMore informations on the sidecar mode can be found on this page.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/usage/sidecar_mode/",
	"title": "Sidecar Mode",
	"tags": [],
	"description": "",
	"content": "Sidecar Mode is a way of deploying Reaper for Apache Cassandra with one reaper instance for each node in the cluster. The name \u0026ldquo;Sidecar\u0026rdquo; comes from the Sidecar Pattern which describes a mechanism for co-locating an auxiliary service with its supported application. See also Design Patterns for Container-based Distributed Systems. It is a pattern that is often used in Kubernetes, where the main application and the sidecar application are deployed as separate containers in the same pod.\nIn Sidecar Mode, each Cassandra node process is deployed alongside a Reaper process; Cassandra is the parent application and Reaper is the sidecar.\nGuidance Deploy one Reaper cluster per Cassandra cluster: Sidecar mode has been designed to allow you to easily deploy a separate, highly available Reaper service for each of your Cassandra clusters. You currently cannot use a Sidecar Mode Reaper service to manage multiple clusters. Deploy Reaper in a sidecar container alongside Cassandra: If you are using Kubernetes to deploy Cassandra and Reaper, put the Reaper process in the same Pod as Cassandra, in a sidecar container. See Pods that run multiple containers that need to work together Use Spreaper and the REST API for Reaper administration (use the WebUI for reporting): If you have automated the deployment of Reaper in Sidecar Mode alongside a Cassandra Cluster (described above). Clusters in sidecar mode are self registered into Reaper, and you can turn on auto-scheduling to fully automate Reaper setup. Use or write an operator: For example, you could write a Kubernetes Custom Resource Definition (e.g. a Cassandra resource) which contains configuration fields for the Cassandra database and fields with which you can define the Reaper repair settings. Then write an accompanying Kubernetes Operator which watches for changes to those resources and reconciles the declared (desired) configuration with the actual state of the Cassandra cluster and its Reaper service. Caveats One Reaper cluster per Cassandra Cluster: Sidecar Mode is designed to be used in situations where you deploy a separate Reaper cluster for every Cassandra cluster. If you prefer to deploy a single Reaper cluster to manage multiple Cassandra clusters then Sidecar Mode may not be suitable for you. High Resource Usage: In Sidecar Mode you will be deploying an additional Java process alongside each Cassandra process. If your underlying infrastructure has limited resources, then Sidecar Mode may not be suitable for you. WebUI Session Affinity: In Sidecar Mode, you will be able to connect to the Web based administration pages of any Reaper process in the cluster. But even once you have logged into the Web UI of one Reaper process, you will not be able to log into the Web UI on other Reaper processes. The Web UI session information is not stored in the underlying Cassandra Backend database. You will need to setup a loadbalancer / service that ensures that all Web UI traffic from a particular client is directed to a particular Reaper process. Snapshot support: In Sidecar Mode, snapshot are not supported as Reaper currently tries to connect to all nodes directly for that feature. "
},
{
	"uri": "http://cassandra-reaper.io/docs/configuration/authentication/",
	"title": "Authentication",
	"tags": [],
	"description": "",
	"content": "Authentication in Reaper is powered by Dropwizard Authentication with JWT (JSON Web Token) support. This provides a modern, stateless authentication system suitable for both web UI and REST API access.\nOverview Reaper implements a dual authentication strategy:\nJWT Authentication: Used for REST API endpoints and modern web applications Basic Authentication: Available as a fallback for simple integrations WebUI Protection: Custom servlet filter that protects the web interface Authentication is enabled by default but requires explicit user configuration. No default users are provided for security reasons - you must configure at least one user or authentication will fail to start.\nConfiguration Authentication is configured in the accessControl section of your YAML configuration:\naccessControl: enabled: true # Enable/disable authentication sessionTimeout: PT10M # Session/token timeout (ISO 8601 duration) jwt: secret: \u0026#34;your-jwt-secret-key\u0026#34; # JWT signing secret (minimum 256 bits for HS256) users: - username: \u0026#34;admin\u0026#34; # REQUIRED: Must not be empty password: \u0026#34;secure-password\u0026#34; # REQUIRED: Must not be empty roles: [\u0026#34;operator\u0026#34;] # REQUIRED: Must have at least one role - username: \u0026#34;monitoring\u0026#34; password: \u0026#34;another-secure-password\u0026#34; roles: [\u0026#34;user\u0026#34;]  Security Notice: You must configure at least one user with a non-empty password, or Reaper will fail to start. Never use default or weak passwords in production.\nConfiguration Parameters enabled Type: Boolean Default: true Description: Enables or disables authentication globally sessionTimeout Type: ISO 8601 Duration Default: PT10M (10 minutes) Description: Session timeout that applies to both WebUI authentication and JWT token expiration. This is the primary timeout setting. jwt.secret Type: String Required: Yes (when JWT is used) Description: Secret key for signing JWT tokens. Must be at least 256 bits (32 characters) for HS256 algorithm Security: Use a cryptographically strong random string in production users Type: Array of user objects Description: In-memory user store configuration User Object Properties username: String - User login name (required, cannot be empty) password: String - User password (required, cannot be empty, stored in plain text) roles: Array of strings - User roles (required, must contain at least one role: [\u0026quot;user\u0026quot;, \u0026quot;operator\u0026quot;]) User Validation Reaper validates all user configurations on startup and will fail to start if:\nNo users are configured when authentication is enabled Any user has an empty or missing username Any user has an empty or missing password Any user has no roles assigned This ensures that weak or incomplete authentication configurations are caught early.\nConditional User Configuration Reaper supports conditional configuration of additional users through environment variables and Docker configuration scripts. This approach allows you to:\nAlways configure the primary admin user via REAPER_AUTH_USER and REAPER_AUTH_PASSWORD (required) Optionally configure a read-only user by setting both REAPER_READ_USER and REAPER_READ_USER_PASSWORD environment variables How It Works The read-only user is configured dynamically at container startup:\nIf both REAPER_READ_USER and REAPER_READ_USER_PASSWORD are set to non-empty values, a read-only user with the [\u0026quot;user\u0026quot;] role is automatically added to the configuration If either variable is empty or unset, no read-only user is configured This prevents environment variable resolution errors while maintaining security Benefits No mandatory unused environment variables: You don\u0026rsquo;t need to set empty values for unused users Flexible deployment: Same Docker image can be used with or without read-only users Security by default: Empty or missing credentials don\u0026rsquo;t create weak authentication points User Roles Reaper implements role-based access control with two main roles:\nuser Role Read-only access to all resources Can view clusters, repair runs, schedules, and metrics Cannot create, modify, or delete resources Suitable for monitoring and reporting users operator Role Full access to all resources Can create, modify, and delete clusters, repair runs, and schedules Can trigger repairs and manage all Reaper functionality Suitable for administrators and operational users Environment Variable Configuration For production deployments, especially with Docker, use environment variables instead of hardcoded values:\naccessControl: enabled: ${REAPER_AUTH_ENABLED:-true} sessionTimeout: ${REAPER_SESSION_TIMEOUT:-PT10M} jwt: secret: \u0026#34;${JWT_SECRET:-MySecretKeyForJWTWhichMustBeLongEnoughForHS256Algorithm}\u0026#34; users: - username: \u0026#34;${REAPER_AUTH_USER}\u0026#34; password: \u0026#34;${REAPER_AUTH_PASSWORD}\u0026#34; roles: [\u0026#34;operator\u0026#34;] # Additional read-only user is configured automatically if REAPER_READ_USER # and REAPER_READ_USER_PASSWORD environment variables are set Docker Configuration Environment Variables When running Reaper in Docker, set these environment variables:\n# Authentication control REAPER_AUTH_ENABLED=true # JWT Configuration JWT_SECRET=\u0026#34;your-production-jwt-secret-key-here\u0026#34; # Admin user credentials - REQUIRED when authentication is enabled REAPER_AUTH_USER=\u0026#34;admin\u0026#34; REAPER_AUTH_PASSWORD=\u0026#34;your-secure-admin-password-here\u0026#34; # Read-only user credentials - OPTIONAL (only configured if both are set) REAPER_READ_USER=\u0026#34;monitoring\u0026#34; REAPER_READ_USER_PASSWORD=\u0026#34;your-secure-monitoring-password-here\u0026#34; # Session timeout REAPER_SESSION_TIMEOUT=\u0026#34;PT30M\u0026#34;  Important: You must set REAPER_AUTH_USER and REAPER_AUTH_PASSWORD, or Reaper will fail to start. The read-only user (REAPER_READ_USER and REAPER_READ_USER_PASSWORD) is optional - it will only be configured if both environment variables are set to non-empty values.\nDocker Compose Example version: \u0026#39;3.8\u0026#39; services: cassandra-reaper: image: cassandra-reaper:latest ports: - \u0026#34;8080:8080\u0026#34; - \u0026#34;8081:8081\u0026#34; environment: # Authentication REAPER_AUTH_ENABLED: \u0026#34;true\u0026#34; JWT_SECRET: \u0026#34;MyProductionJWTSecretKeyThatIsLongEnoughForHS256\u0026#34; # User credentials - CHANGE THESE! REAPER_AUTH_USER: \u0026#34;admin\u0026#34; REAPER_AUTH_PASSWORD: \u0026#34;change-this-secure-password\u0026#34; # Optional read-only user (remove these lines to disable) REAPER_READ_USER: \u0026#34;monitoring\u0026#34; REAPER_READ_USER_PASSWORD: \u0026#34;change-this-monitoring-password\u0026#34; # Session configuration REAPER_SESSION_TIMEOUT: \u0026#34;PT30M\u0026#34; # Storage configuration REAPER_STORAGE_TYPE: \u0026#34;cassandra\u0026#34; REAPER_CASS_CONTACT_POINTS: \u0026#34;[\\\u0026#34;cassandra:9042\\\u0026#34;]\u0026#34; depends_on: - cassandra "
},
{
	"uri": "http://cassandra-reaper.io/docs/api/",
	"title": "Rest API",
	"tags": [],
	"description": "",
	"content": "Source code for all the REST resources can be found from package io.cassandrareaper.resources.\nLogin Resource POST /login Expected form parameters:\n* username : User to login with as defined in authentication settings (default user is admin)\n* password : Password to authenticate with (default password of user admin is: admin)\n* rememberMe : Boolean to have the Web UI remember the username (Optional) Endpoint for logging in to Reaper. Returns a JWT token in the response that must be passed in the Authorization HTTP header for authenticated requests: Authorization: Bearer [JWT value] Ping Resource GET /ping\nExpected query parameters: None Simple ping resource that can be used to check whether the reaper is running. HEAD /ping\nExpected query parameters: None Health check endpoint that returns HTTP 204 if healthy, 500 if not. Cluster Resource GET /cluster\nExpected query parameters: seedHost: Limit the returned cluster list based on the given seed host. (Optional) Returns a list of registered cluster names in the service. GET /cluster/{cluster_name}\nExpected query parameters: limit: Limit the number of repair runs returned. Recent runs are prioritized. (Optional) Returns a cluster object identified by the given \u0026ldquo;cluster_name\u0026rdquo; path parameter. GET /cluster/{cluster_name}/tables\nExpected query parameters: None Returns a map of \u0026lt;KeyspaceName, List\u0026lt;TableName\u0026gt;\u0026gt; POST /cluster\nExpected query parameters: seedHost: Host name or IP address of the added Cassandra clusters seed host. jmxPort: A custom port that reaper will use to JMX into the Cluster. Defaults to 7199. (Optional) Adds a new cluster to the service, and returns the newly added cluster object, if the operation was successful. If the cluster is already registered, the list of hosts will be updated to match the current topology. POST /cluster/auth\nExpected form parameters: seedHost: Host name or IP address of the added Cassandra clusters seed host. jmxPort: A custom port that reaper will use to JMX into the Cluster. Defaults to 7199. (Optional) jmxUsername: JMX Username specific to the Cluster. Defaults to what is defined in configuration. (Optional) jmxPassword: JMX Password specific to the Cluster. Defaults to what is defined in configuration. (Optional) Adds a new cluster to the service, and returns the newly added cluster object, if the operation was successful. If the cluster is already registered, the list of hosts will be updated to match the current topology. PUT /cluster/{cluster_name}\nExpected query parameters: seedHost: New host name or IP address used as Cassandra cluster seed. jmxPort: A custom port that reaper will use to JMX into the Cluster. Defaults to 7199. (Optional) Adds a new cluster or modifies an existing cluster\u0026rsquo;s seed host. Comes in handy when the previous seed has left the cluster. PUT /cluster/auth/{cluster_name}\nExpected form parameters: seedHost: New host name or IP address used as Cassandra cluster seed. jmxPort: A custom port that reaper will use to JMX into the Cluster. Defaults to 7199. (Optional) jmxUsername: JMX Username specific to the Cluster. Defaults to what is defined in configuration. (Optional) jmxPassword: JMX Password specific to the Cluster. Defaults to what is defined in configuration. (Optional) Adds a new cluster or modifies an existing cluster\u0026rsquo;s seed host. Comes in handy when the previous seed has left the cluster. DELETE /cluster/{cluster_name}\nExpected query parameters: force: Enforce deletion of the cluster even if there are active schedules and a repair run history (Optional) Delete a cluster object identified by the given \u0026ldquo;cluster_name\u0026rdquo; path parameter. Cluster will get deleted only if there are no schedules or repair runs for the cluster, or the request will fail. Delete repair runs and schedules first before calling this. Repair Run Resource GET /repair_run\nOptional query parameters: state: Comma separated list of repair run state names. Only names found in io.cassandrareaper.core.RunState are accepted. (Optional) cluster_name: Name of the Cassandra cluster. (Optional) keyspace_name: The name of the table keyspace. (Optional) limit: Limit the number of repair runs returned. (Optional) Returns a list of repair runs, optionally fetching only the ones with specified filters. GET /repair_run/{id}\nExpected query parameters: None Returns a repair run object identified by the given \u0026ldquo;id\u0026rdquo; path parameter. GET /repair_run/cluster/{cluster_name}\nExpected query parameters: limit: Limit the number of repair runs returned. (Optional) Returns a list of all repair run statuses found for the given \u0026ldquo;cluster_name\u0026rdquo; path parameter. GET /repair_run/{id}/segments\nExpected query parameters: None Returns the list of segments of the repair run. POST /repair_run/{id}/segments/abort/{segment_id}\nExpected query parameters: None Aborts a running segment and puts it back in NOT_STARTED state. The segment will be processed again later during the lifetime of the repair run. POST /repair_run\nExpected query parameters: * clusterName: Name of the Cassandra cluster. * keyspace: The name of the table keyspace. * tables: The name of the targeted tables (column families) as comma separated list. If no tables given, then the whole keyspace is targeted. (Optional) * owner: Owner name for the run. This could be any string identifying the owner. * cause: Identifies the process, or cause the repair was started. (Optional) * segmentCountPerNode: Defines the amount of segments per node to create for the repair run. (Optional) * repairParallelism: Defines the used repair parallelism for repair run. (Optional) * intensity: Defines the repair intensity for repair run. (Optional) * incrementalRepair: Defines if incremental repair should be done. [true/false] (Optional) * subrangeIncrementalRepair: Defines if subrange incremental repair should be done. [true/false] (Optional) * nodes: a specific list of nodes whose tokens should be repaired. (Optional) * datacenters: a specific list of datacenters to repair. (Optional) * blacklistedTables: The name of the tables that should not be repaired. Cannot be used in conjunction with the tables parameter. (Optional) * repairThreadCount: Since Cassandra 2.2, repairs can be performed with up to 4 threads in order to parallelize the work on different token ranges. (Optional) * force: Force the repair even if there are validation errors. (Optional) * timeout: Timeout in seconds for repair operations. (Optional) Endpoint used to create a repair run. Does not allow triggering the run. Creating a repair run includes generating the repair segments. Notice that query parameter \u0026ldquo;tables\u0026rdquo; can be a single String, or a comma-separated list of table names. If the \u0026ldquo;tables\u0026rdquo; parameter is omitted, and only the keyspace is defined, then created repair run will target all the tables in the keyspace. Returns the ID of the newly created repair run if successful. PUT /repair_run/{id}/state/{state}\nExpected query parameters: None Starts, pauses, or resumes a repair run identified by the \u0026ldquo;id\u0026rdquo; path parameter.\nCan also be used to reattempt a repair run in state \u0026ldquo;ERROR\u0026rdquo;, picking up where it left off.\nPossible values for given state are: \u0026ldquo;PAUSED\u0026rdquo;, \u0026ldquo;RUNNING\u0026rdquo;, or \u0026ldquo;ABORTED\u0026rdquo;. PUT /repair_run/{id}/intensity/{intensity}\nExpected query parameters: None Modifies the intensity of a PAUSED repair run. Returns OK if all goes well NOT_MODIFIED if new state is the same as the old one, and 409 (CONFLICT) if transition is not supported. DELETE /repair_run/{id}\nExpected query parameters: owner: Owner name for the run. If the given owner does not match the stored owner, the delete request will fail. (Optional) Delete a repair run object identified by the given \u0026ldquo;id\u0026rdquo; path parameter. Repair run and all the related repair segments will be deleted from the database. POST /repair_run/purge\nExpected query parameters: None Purge completed repair runs from the database based on configured retention policies. Repair Schedule Resource GET /repair_schedule\nExpected query parameters: clusterName: Filter the returned schedule list based on the given cluster name. (Optional) keyspace: Filter the returned schedule list based on the given keyspace name. (Optional) Returns all repair schedules present in the Reaper GET /repair_schedule/{id}\nExpected query parameters: None Returns a repair schedule object identified by the given \u0026ldquo;id\u0026rdquo; path parameter. GET /repair_schedule/cluster/{cluster_name}\nExpected query parameters: None Returns the repair schedule objects for the given cluster. POST /repair_schedule\nExpected query parameters: * clusterName: Name of the Cassandra cluster. * keyspace: The name of the table keyspace. * tables*: The name of the targeted tables (column families) as comma separated list. If no tables given, then the whole keyspace is targeted. (Optional) * owner: Owner name for the schedule. This could be any string identifying the owner. * segmentCountPerNode: Defines the amount of segments per node to create for scheduled repair runs. (Optional) * repairParallelism: Defines the used repair parallelism for scheduled repair runs. (Optional) * intensity: Defines the repair intensity for scheduled repair runs. (Optional) * incrementalRepair: Defines if incremental repair should be done on all tokens of each node at once. [true/false] (Optional) subrangeIncrementalRepair*: Defines if incremental repair should be done in subrange mode, against discrete token ranges. [true/false] (Optional) scheduleDaysBetween: Defines the amount of days to wait between scheduling new repairs. For example, use value 7 for weekly schedule, and 0 for continuous. scheduleTriggerTime: Defines the time for first scheduled trigger for the run. If you don\u0026rsquo;t give this value, it will be next mid-night (UTC). Give date values in ISO format, e.g. \u0026ldquo;2015-02-11T01:00:00\u0026rdquo;. (Optional) nodes: a specific list of nodes whose tokens should be repaired. (Optional) datacenters: a specific list of datacenters to repair. (Optional) blacklistedTables: The name of the tables that should not be repaired. Cannot be used in conjunction with the tables parameter. (Optional) repairThreadCount: Since Cassandra 2.2, repairs can be performed with up to 4 threads in order to parallelize the work on different token ranges. (Optional) force: Force the repair even if there are validation errors. (Optional) timeout: Timeout in seconds for repair operations. (Optional) adaptive: Enable adaptive scheduling based on repair metrics. [true/false] (Optional) percentUnrepairedThreshold: Threshold of unrepaired percentage that triggers a repair. (Optional) Create and activate a scheduled repair. PATCH /repair_schedule/{id}\nExpected JSON body with repair schedule parameters: owner: Owner name for the schedule. (Optional) repairParallelism: Repair parallelism setting. (Optional) intensity: Repair intensity value. (Optional) daysBetween: Days between scheduled repairs. (Optional) segmentCountPerNode: Number of segments per node. (Optional) adaptive: Enable adaptive scheduling. (Optional) percentUnrepairedThreshold: Percentage threshold for repairs. (Optional) Update specific fields of an existing repair schedule. POST /repair_schedule/start/{id}\nExpected query parameters: None Force start a repair from a schedule. DELETE /repair_schedule/{id}\nExpected query parameters: owner: Owner name for the schedule. If the given owner does not match the stored owner, the delete request will fail. (Optional) Delete a repair schedule object identified by the given \u0026ldquo;id\u0026rdquo; path parameter. Repair schedule will get deleted only if there are no associated repair runs for the schedule. Delete all the related repair runs before calling this endpoint. PUT /repair_schedule/{id}\nExpected query parameters: state: \u0026ldquo;PAUSED\u0026rdquo; or \u0026ldquo;ACTIVE\u0026rdquo;. Enables or disables the repair schedule. GET /repair_schedule/{clusterName}/{id}/percent_repaired\nExpected query parameters: None Returns percent repaired metrics for the specified repair schedule. Snapshot Resource GET /snapshot/cluster/{clusterName}/{host}\nExpected query parameters: None Lists snapshots for the given host in the given cluster. GET /snapshot/cluster/{clusterName}\nExpected query parameters: None Lists all snapshots for the given cluster. DELETE /snapshot/cluster/{clusterName}/{host}/{snapshotName}\nExpected query parameters: None Deletes a specific snapshot on a given node. DELETE /snapshot/cluster/{clusterName}/{snapshotName}\nExpected query parameters: None Deletes a specific snapshot on all nodes in a given cluster. POST /snapshot/cluster/{clusterName}\nExpected query parameters: * keyspace: Name of the keyspace to snapshot. * snapshot_name: name to use for the snapshot. (Optional) * owner: Owner name for the snapshot. This could be any string identifying the owner. (Optional) * cause: Identifies the process, or cause the snapshot was created. (Optional) Create a snapshot on all hosts in a cluster, using the same name. POST /snapshot/cluster/{clusterName}/{host}\nExpected query parameters: * keyspace: Name of the keyspace to snapshot. * tables: The name of the targeted tables (column families) as comma separated list. If no tables given, then the whole keyspace is targeted. (Optional) * snapshot_name: name to use for the snapshot. (Optional) Create a snapshot on a specific host. Crypto Resource GET /crypto/encrypt/{text} Expected query parameters: None Path parameter: * text: The text to encrypt. Encrypt text when cryptograph settings are configured in the reaper yaml. Node Resource GET /node/tpstats/{clusterName}/{host}\nExpected query parameters: None Returns thread pool stats for a node. GET /node/dropped/{clusterName}/{host}\nExpected query parameters: None Returns dropped messages stats for a node. GET /node/clientRequestLatencies/{clusterName}/{host}\nExpected query parameters: None Returns client request latencies for a node. GET /node/compactions/{clusterName}/{host}\nExpected query parameters: None Returns active compactions for a node. GET /node/tokens/{clusterName}/{host}\nExpected query parameters: None Returns the tokens owned by a node. Reaper Resource GET / Expected query parameters: None Returns basic information about the Reaper instance. "
},
{
	"uri": "http://cassandra-reaper.io/docs/community/",
	"title": "Community",
	"tags": [],
	"description": "",
	"content": "Join the Apache Software Foundation Slack and then reach out on the #cassandra-reaper channel.\n"
},
{
	"uri": "http://cassandra-reaper.io/faq/",
	"title": "Frequently Asked Questions",
	"tags": [],
	"description": "",
	"content": "Frequently Asked Questions Why use Reaper instead of nodetool + cron? While it\u0026rsquo;s possible to set up crontab to call nodetool, it requires staggering the crons to ensure overlap is kept to a minimum. Reaper is able to intelligently schedule repairs to avoid putting too much load on the cluster, avoiding impacting performance. Reaper also offers a simple UI to schedule repairs as granularly as needed.\nDo I need to do repairs if I\u0026rsquo;m not deleting data? Yes! Repair is a necessary anti-entropy mechanism that keeps your cluster consistent. Without repair, queries at LOCAL_ONE could return incorrect results.\nWhich backend is used to store Reaper\u0026rsquo;s data? When we (The Last Pickle) took over development of Reaper, we found it cumbersome to require a PostGres database in addition to the Cassandra database. We also knew Reaper would need to be fault tolerant and work across datacenters. The most straightforward way to do this would be to leverage Cassandra\u0026rsquo;s fault tolerance.\nPostgres and H2 were removed starting with v3.0.0 in order to simplify the codebase and feature addition. The memory backend is still available for testing purposes.\n"
},
{
	"uri": "http://cassandra-reaper.io/docs/development/",
	"title": "Development",
	"tags": [],
	"description": "",
	"content": "Cutting Releases Cutting a release involves the following steps.\nConsult with the community to confirm the codebase is ready for release by reviewing outstanding issues and pull requests.\nFor major releases, create a release branch from master following the naming convention: 1.0, 1.1, 1.2, 1.3, etc.\nGenerate the changelog by running:\ngithub-changes -o thelastpickle -r cassandra-reaper --use-commit-body -a -f changelog.tmp -b \u0026lt;sha-of-release\u0026gt; -v Review and edit the generated changelog to include only relevant commits, then update CHANGELOG.md.\nUpdate the version number using:\nmvn -B versions:set \u0026#34;-DnewVersion=\u0026lt;release-version-number\u0026gt;\u0026#34; Version numbers should follow semantic versioning: \u0026lt;major\u0026gt;.\u0026lt;minor\u0026gt;.\u0026lt;patch\u0026gt; or \u0026lt;major\u0026gt;.\u0026lt;minor\u0026gt;.\u0026lt;patch\u0026gt;-\u0026lt;prerelease suffix\u0026gt;.\nCreate and push a tag matching the release number (omit the v prefix).\nNavigate to the GitHub tags page and publish a release from the new tag.\nUpdate the GitHub release description with the latest version\u0026rsquo;s changelog entries.\nMonitor the GitHub Actions workflow and verify CI completion for the release.\nForward merge the release changelog commit to master branch as detailed in the following section to maintain branches synchronization.\nForward Merging Reaper practices forward merging commits.\nFixes and improvements required to release branches are first committed to those branches. These changes are merged forward onto master afterwards.\nAn example where a bugfix developed and approved for the release branch 1.4 is to be merged;\n# switch to master git checkout master # create the empty merge commit git merge 1.4 -s ours # cherry-pick the change using the sha noted above, and make any manual adjustments required against the master branch git cherry-pick -n \u0026lt;sha\u0026gt; # commit amend the forward ported changes into the merge commit git commit -a --amend # push both 1.4 and master branches at the same time git push origin 1.4 master --atomic For more information on the value of forward merging, and the principles of \u0026ldquo;merge down, copy up\u0026rdquo; and the \u0026ldquo;tofu scale: firm above, soft below\u0026rdquo;, see\nhttps://www.perforce.com/perforce/conferences/us/2005/presentations/Wingerd.pdf https://www.youtube.com/watch?v=AJ-CpGsCpM0 Releasing Docs The cassandra-reaper.io pages are not updated automatically when doing a release. Deploying new docs is a manual process for now. To do the deploy:\nEnsure you\u0026rsquo;re on master branch that\u0026rsquo;s up to date with the upstream repo. Navigate to the src/docs. Run make build. Commit changes in the docs folder (relative to the root of the repository). Open a PR with the changes. Editor Config Reaper uses Spotless to enforce consistent code style across the repository. The Google Code Style guidelines are automatically applied to all files when running mvn spotless:apply.\nCode style compliance is verified during the Maven build process. You can also manually check for style violations by running mvn spotless:check.\n"
},
{
	"uri": "http://cassandra-reaper.io/quickstart/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://cassandra-reaper.io/",
	"title": "Reaper for Apache Cassandra",
	"tags": [],
	"description": "",
	"content": "Reaper: Easy Repair Management for Apache Cassandra Reaper is a centralized, stateful, and highly configurable tool for running Apache Cassandra repairs for multi-site clusters.\nWhy Use Reaper? Instead of manually managing crontabs and nodetool commands, Reaper provides:\nCentralized Management: One place to manage repairs across all your Cassandra clusters Intelligent Coordination: Prevents overlapping repairs that could impact cluster performance Fault Tolerance: Built-in retry logic and state management for reliable repair execution Granular Control: Schedule repairs at keyspace, table, or even token range level Multi-Site Awareness: Coordinate repairs across datacenters intelligently Key Features  Intelligent Scheduling: Automatically schedules repairs to avoid putting too much load on your cluster  Web Interface: Simple and intuitive web UI to schedule repairs as granularly as needed  Multi-Backend Support: Store Reaper\u0026rsquo;s data in Cassandra itself for fault tolerance  Multi-Datacenter: Built for multi-site clusters with cross-datacenter repair coordination  Monitoring \u0026amp; Metrics: Comprehensive metrics and monitoring integration  REST API: Full REST API for programmatic control and integration Quick Start Using Docker docker run -p 8080:8080 thelastpickle/cassandra-reaper:latest Download JAR Download the latest JAR from the releases page Create a configuration file Run: java -jar cassandra-reaper-*.jar server reaper.yaml Build from Source Clone the repository: git clone https://github.com/thelastpickle/cassandra-reaper.git Build: mvn clean package Run: java -jar target/cassandra-reaper-*.jar server src/main/resources/cassandra-reaper.yaml Documentation Explore the documentation to learn how to:\nInstall and configure Reaper in your environment Use the web interface to manage repairs Configure backends for storing Reaper\u0026rsquo;s data Set up monitoring and metrics collection Use the REST API for automation Reaper is developed and maintained by The Last Pickle and the open source community.\n"
}]